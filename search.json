[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Handling Missing Values\n\n\n\n\n\n\nMachine Learning\n\n\nFeature Engineering\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nThe Art of Readiable R code\n\n\n\n\n\n\nGood Practices\n\n\nCoding Style\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nAngel Feliz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html",
    "href": "posts/02-imputing-missing-values/main.html",
    "title": "Handling Missing Values",
    "section": "",
    "text": "If you are working with real data, it’s normal to find missing values, so it’s very important to understand how to manage them correctly. In R, the default for many functions is to remove missing values to create plots or train machine learning models, but that can be very dangerous as we are adding bias to our analysis that could compromise our final conclusions.\nBut removing values could be a good option if we meet the following statements:\n\nOnly 5% of your dataset’s rows present missing values.\nAll the values have the same probability to be missing. That’s when we say that they are missing completely at random (MCAR).\n\nOtherwise, the best way to handle missing values is to impute values based on general patterns found in the data. If we cannot find patterns in the current data, we will need to find more data until finding a valid pattern to impute the values."
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#introduction",
    "href": "posts/02-imputing-missing-values/main.html#introduction",
    "title": "Handling Missing Values",
    "section": "",
    "text": "If you are working with real data, it’s normal to find missing values, so it’s very important to understand how to manage them correctly. In R, the default for many functions is to remove missing values to create plots or train machine learning models, but that can be very dangerous as we are adding bias to our analysis that could compromise our final conclusions.\nBut removing values could be a good option if we meet the following statements:\n\nOnly 5% of your dataset’s rows present missing values.\nAll the values have the same probability to be missing. That’s when we say that they are missing completely at random (MCAR).\n\nOtherwise, the best way to handle missing values is to impute values based on general patterns found in the data. If we cannot find patterns in the current data, we will need to find more data until finding a valid pattern to impute the values."
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#imputation-practical-example",
    "href": "posts/02-imputing-missing-values/main.html#imputation-practical-example",
    "title": "Handling Missing Values",
    "section": "2 Imputation practical example",
    "text": "2 Imputation practical example\nLet’s assume that all the predictors for a machine learning model are stored in the datasets::airquality data frame, which has some missing values, so we need to explore and decide what to do in this case.\nTo perform all the tasks needed to solve the missing values problems in our dataset, we will load the following packages:\n\n# For modeling and statistical analysis\nlibrary(tidymodels)\n\n# For modeling lasso or elastic-net regression\nlibrary(glmnet)\n\n# For exploring missing values\nlibrary(naniar)\nlibrary(mice)\n\n\n2.1 Confirming if values are MCAR\nBased on the next test, we can reject the null hypothesis and conclude that the missing values aren’t completely at random, so we will need to impute the missing values.\n\nmcar_test(airquality)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      35.1    14 0.00142                4\n\n\n\n\n2.2 Explore missing values patterns\nOnce we know that we need to impute values, it’s important to know that the column with more missing values is the Ozone with 37 values to impute, which we can divide between the ones that can use the rest of the features (35 rows) and the ones that can use all the columns except the Solar.R as they are missing (2 rows).\n\nairquality |&gt;\n  md.pattern() |&gt;\n  invisible()\n\n\n\n\n\n\n\n\n\n\n2.3 Imputing Ozone values\n\n2.3.1 Exploring missing values\nIn the next plot we can see how the missing Ozone values are spread across a wide range of values, so we wouldn’t be able to find big differences between the means of columns with or without missing Ozone values.\nAs we are plotting all variables against the target variable, it’s easy to see that the Temp, Wind and Solar.R present a not linear relation with Ozone and we cannot see a clear pattern for Day and Month.\n\nairquality |&gt;\n  pivot_longer(cols = -Ozone) |&gt;\n  ggplot(aes(value, Ozone))+\n  geom_miss_point(aes(color = is.na(Ozone)),\n                  alpha = 0.5,\n                  show.legend = FALSE)+\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"black\")+\n  scale_color_manual(values = c(\"TRUE\" = \"red\",\n                                \"FALSE\" = \"gray60\"))+\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_light()\n\n\n\n\n\n\n\n\nBased on this result, we know it’s necessary to train a model able to catch the non-linear patterns and omit the Day and Month as they don’t show any relation with Ozone.\n\n\n2.3.2 Training the model to impute\nAs we need to create 2 very similar models (one using the Solar.R column and other without it) it’s better to create a function.\n\nfit_tuned_regression_model &lt;- function(df,\n                                       model_to_tune,\n                                       model_grid,\n                                       formula,\n                                       step_fun,\n                                       metric = rsq,\n                                       seed = 1234){\n  \n  # Defining empty model workflow\n  y_wf &lt;- \n    recipe(formula, data = df) |&gt;\n    step_fun() |&gt;\n    workflow(model_to_tune)\n  \n  # Defining re-samples based on seed\n  set.seed(seed)\n  y_resamples &lt;- mc_cv(df, times = 30)\n  set.seed(NULL)\n  \n  # Fitting re-sample for each grid level\n  y_tuned &lt;- tune_grid(\n    y_wf,\n    resamples = y_resamples,\n    grid = model_grid,\n    metrics = metric_set(metric)\n  )\n  \n  # Selecting the best model\n  y_trained_model &lt;- \n    finalize_workflow(y_wf,\n                      select_best(y_tuned)) |&gt;\n    fit(df)\n  \n  # Presenting results\n  results &lt;- list(\"fit\" = y_trained_model,\n                  \"best_fit\" = show_best(y_tuned))\n  \n  print(results$best_fit)\n  \n  return(results)\n  \n}\n\nThen we need to define common inputs for both models.\n\nGlmModel &lt;- linear_reg(\n  engine = \"glmnet\",\n  penalty = tune(),\n  mixture = tune()\n)\n\nGlmGrid &lt;- grid_regular(\n  penalty(),\n  mixture(),\n  levels = 10\n)\n\nozone_steps &lt;- function(recipe){\n  \n  recipe |&gt;\n  step_poly(all_numeric_predictors(),\n            degree = 2) |&gt;\n    step_interact(terms = ~(. -Ozone)^2) |&gt;\n    step_zv(all_numeric_predictors()) |&gt;\n    step_scale(all_numeric_predictors())\n  \n}\n\nNow we can fit each model.\n\nOzoneSolarGlmFitted &lt;- fit_tuned_regression_model(\n  df = na.omit(airquality),\n  model_to_tune = GlmModel,\n  model_grid = GlmGrid,\n  formula = as.formula(\"Ozone ~ Solar.R + Temp + Wind\"),\n  step_fun = ozone_steps,\n  seed = 5050\n)\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 1               1     rsq     standard   0.721    30  0.0159 Preprocessor1_Mo…\n2 1               0.889 rsq     standard   0.721    30  0.0157 Preprocessor1_Mo…\n3 1               0.778 rsq     standard   0.720    30  0.0156 Preprocessor1_Mo…\n4 0.0000000001    0     rsq     standard   0.718    30  0.0165 Preprocessor1_Mo…\n5 0.00000000129   0     rsq     standard   0.718    30  0.0165 Preprocessor1_Mo…\n\nOzoneNotSolarGlmFitted &lt;- airquality |&gt;\n  select(-Solar.R) |&gt;\n  na.omit() |&gt;\n  fit_tuned_regression_model(model_to_tune = GlmModel,\n                         model_grid = GlmGrid,\n                         formula = as.formula(\"Ozone ~ Temp + Wind\"),\n                         step_fun = ozone_steps,\n                         seed = 4518)\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001        0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n2 0.00000000129       0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n3 0.0000000167        0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n4 0.000000215         0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n5 0.00000278          0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n\n\n\n\n2.3.3 Impute missing values\nOnce we have both models, we can impute the Ozone values, but let’s also create a function to perform this task as we will need to repeat the process very time we need to predict a new value.\n\nimpute_ozone &lt;- function(df,\n                         solar_model,\n                         no_solar_model){\n  \n  mutate(df,\n         Ozone_NA = is.na(Ozone),\n         Ozone = case_when(\n           !is.na(Ozone) ~ Ozone,\n           !is.na(Solar.R)~\n             predict(solar_model, new_data = df)$.pred,\n           TRUE ~\n             predict(no_solar_model, new_data = df)$.pred)\n  )\n}\n\n\nAirOzoneImputed &lt;- impute_ozone(\n  airquality,\n  solar_model = OzoneSolarGlmFitted$fit,\n  no_solar_model = OzoneNotSolarGlmFitted$fit\n)\n\nBy plotting the imputed Ozone values can see that the values follow the patterns present in the non-missing values.\n\nAirOzoneImputed |&gt;\n  pivot_longer(cols = -c(Ozone, Ozone_NA)) |&gt;\n  na.omit() |&gt;\n  ggplot(aes(value, Ozone, color = Ozone_NA))+\n  geom_point(show.legend = FALSE)+\n  scale_color_manual(values = c(\"TRUE\" = \"red\",\n                                \"FALSE\" = \"grey60\"))+\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n\n2.4 Fixing Solar.R values\nOnce we don’t have any missing value in the Ozone column we can explore the remaining 7 missing values in the Solar.R column.\n\nAirOzoneImputed |&gt;\n  md.pattern() |&gt;\n  invisible()\n\n\n\n\n\n\n\n\nNow the missing values represent only 5% of the rows.\n\nnrow(na.omit(AirOzoneImputed)) / nrow(AirOzoneImputed) * 100\n\n[1] 95.42484\n\n\nAnd we don’t have enough data to reject the null hypothesis, and we can not affirm that the missing values are not missing completely at random.\n\nmcar_test(AirOzoneImputed)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      10.3     6   0.114                2\n\n\nSo we can remove the remaining missing values.\n\nAirqualityImputed &lt;- na.omit(AirOzoneImputed)\n\nhead(AirqualityImputed)\n\n  Ozone Solar.R Wind Temp Month Day Ozone_NA\n1    41     190  7.4   67     5   1    FALSE\n2    36     118  8.0   72     5   2    FALSE\n3    12     149 12.6   74     5   3    FALSE\n4    18     313 11.5   62     5   4    FALSE\n7    23     299  8.6   65     5   7    FALSE\n8    19      99 13.8   59     5   8    FALSE"
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#final-thoughts",
    "href": "posts/02-imputing-missing-values/main.html#final-thoughts",
    "title": "Handling Missing Values",
    "section": "3 Final thoughts",
    "text": "3 Final thoughts\nI hope this blog can help to improve your ability to handle missing values without compromising your results.\nDon’t forget to explore well your columns as missing values are usually encoded with 0 or 99."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angel Feliz",
    "section": "",
    "text": "Senior Reporting Analyst with 2 years’ experience using R to process and create business reports in formats as Excel, Word, Power Point, and html.\nProven experience integrating ad hoc regular expressions to transform, extract and validate text data in data pipelines to find abnormalities and automate repetitive tasks; improving reporting accuracy, reducing waiting times and opening new possibilities to empower decision makers.\nAutodidactic, self-motivated, inquisitive, and eager to apply data science techniques to create business value and increase ROI for your company"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Angel Feliz",
    "section": "",
    "text": "Senior Reporting Analyst with 2 years’ experience using R to process and create business reports in formats as Excel, Word, Power Point, and html.\nProven experience integrating ad hoc regular expressions to transform, extract and validate text data in data pipelines to find abnormalities and automate repetitive tasks; improving reporting accuracy, reducing waiting times and opening new possibilities to empower decision makers.\nAutodidactic, self-motivated, inquisitive, and eager to apply data science techniques to create business value and increase ROI for your company"
  },
  {
    "objectID": "posts/01-readiable-code/main.html",
    "href": "posts/01-readiable-code/main.html",
    "title": "The Art of Readiable R code",
    "section": "",
    "text": "When we start our journey as programmers it’s normal to get excited by the new possibilities. We get the capacity to do many things that otherwise would be impossible, making projects faster and assuring consistency.\nBut the problems start when you need to modify a script that you wrote 6 months ago. That’s when you find out that you don’t remember why you were applying some specific filters or calculating a value in a odd way.\nAs a Reporting Analyst and I am always creating and changing scripts and after applying the tips provided in The Art of Readable Code by Dustin Boswell and Trevor Foucher I could reduce the time needed to apply changes from 5 to 2 days (60% faster)."
  },
  {
    "objectID": "posts/01-readiable-code/main.html#creating-explicit-names",
    "href": "posts/01-readiable-code/main.html#creating-explicit-names",
    "title": "The Art of Readiable R code",
    "section": "2.1 Creating explicit names",
    "text": "2.1 Creating explicit names\n\n2.1.1 Naming variables\nDefining good variable names is more important than writing a good comment and we should try to give as much context as possible in the variable name. To make this possible:\n\nName based on variable value.\n\nBoolean variables can use words like is, has and should avoid negations. For example: is_integer, has_money and should_end.\nLooping index can have a name followed by the the suffix i. For example: club_i and table_i.\n\nAdd dimensions unit a suffix. For example: price_usd, mass_kg and distance_miles.\nNever change the variable’s value in different sections, instead create a new variable making explicit the change in the name. For example, we can have the variable priceand latter we can create the variable price_discount.\n\nCoding Example\nLet’s apply these points to the mtcars.\n\nmtcars_new_names &lt;-\n  c(\"mpg\" = \"miles_per_gallon\",\n    \"cyl\"= \"cylinders_count\",\n    \"disp\" = \"displacement_in3\",\n    \"hp\" = \"power_hp\",\n    \"drat\" = \"rear_axle_ratio\",\n    \"wt\" = \"weight_klb\",\n    \"qsec\" = \"quarter_mile_secs\",\n    \"vs\" = \"engine_is_straight\",\n    \"am\" = \"transmission_is_manual\",\n    \"gear\" = \"gear_count\",\n    \"carb\" = \"carburetor_count\")\n\nmtcars_renamed &lt;- mtcars\nnames(mtcars_renamed) &lt;- mtcars_new_names[names(mtcars)]\n\nstr(mtcars_renamed)\n\n'data.frame':   32 obs. of  11 variables:\n $ miles_per_gallon      : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cylinders_count       : num  6 6 4 6 8 6 8 4 4 6 ...\n $ displacement_in3      : num  160 160 108 258 360 ...\n $ power_hp              : num  110 110 93 110 175 105 245 62 95 123 ...\n $ rear_axle_ratio       : num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ weight_klb            : num  2.62 2.88 2.32 3.21 3.44 ...\n $ quarter_mile_secs     : num  16.5 17 18.6 19.4 17 ...\n $ engine_is_straight    : num  0 0 1 1 0 1 0 1 1 1 ...\n $ transmission_is_manual: num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear_count            : num  4 4 4 3 3 3 3 4 4 4 ...\n $ carburetor_count      : num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo write good variable names might take some iteration and you might need to play devil’s advocate in under to find out a better name than the initial one.\n\n\n\n\n2.1.2 Defining functions\nCreating explicit functions names can transform a complex process into a simple one.\n\nStart the function with an explicit verb to avoid misunderstandings.\n\n\n\n\nWord\nAlternatives\n\n\n\n\nsend\ndeliver, dispatch, announce, distribute, route\n\n\nfind\nsearch, extract, locate, recover\n\n\nstart\nlaunch, create, begin, open\n\n\nmake\ncreate, set up, build, generate, compose, add, new\n\n\n\n\nThe function name must describe its output.\nA function should do only one thing, otherwise break the functions in more simpler ones to keep the name explicit.\nUse the following words to define range arguments.\n\n\n\n\n\n\n\n\nWord\nUse\n\n\n\n\nmin and max\nUseful to denominate included limits\n\n\nfirst and last\nUseful to denominate exclusive limits\n\n\nbegin and end\nUseful to denominate either inclusive or exclusive limits\n\n\n\nCoding Example\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n  \n  if(!is.data.frame(DF)) stop(\"DF should be a data.frame\")\n  \n  values &lt;- DF[[var_name]]\n  if(!is.numeric(values)) stop(\"var_name should be a numeric column of DF\")\n  \n  min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n  max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n  \n  value_in_range &lt;- \n    values &gt;= min_value & \n    values &lt;= max_value\n  \n  return(DF[value_in_range, ])\n  \n}"
  },
  {
    "objectID": "posts/01-readiable-code/main.html#commenting-correctly",
    "href": "posts/01-readiable-code/main.html#commenting-correctly",
    "title": "The Art of Readiable R code",
    "section": "2.2 Commenting correctly",
    "text": "2.2 Commenting correctly\nThe first step to have a commented project is to have a README file explaining how the code works in a way that to should be enough to present the project to a new team member, but it is also important to add comments to:\n\nExplain how custom functions behave in several situations with minimal examples.\nExplain the reasons behind the decisions that have been taken related to coding style and business logic, like method and constant selection.\nMake explicit pending problems to solve and the initial idea we have to start the solution.\nAvoid commenting bad names, fix them instead.\nSummarize coding sections with a description faster to read than the original code.\n\nCoding Example\nLet’s comment our custom function to explain each point.\n\n# 1. Behavior\n# This function can filter the values of any  data.frame if the var_name\n# is numeric no matter if the column has missing values as it will omit them\n\n# 2. Reasons behind decisions\n# As we are not expecting to make inferences imputation is not necessary.\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n\n  # 5. Reading the code is faster than reading a comment, so we don't need it\n  if(!is.data.frame(DF)) stop(\"DF should be a data.frame\")\n\n  # 2. Reasons behind decisions\n  # We are going to use this vector many times and \n  # saving it as a variable makes the code much easier to read\n  values &lt;- DF[[var_name]]\n  \n  # 5. Reading the code is faster than reading a comment, so we don't need it\n  if(!is.numeric(values)) stop(\"var_name should be a numeric column of DF\")\n  \n  # 2. Reasons behind decisions\n  # Even though a single quantile call could return both values in a vector\n  # it is much simpler to understand if we save each value in a variable\n  min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n  max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n  \n  # 4. The boolean test has an explicit name\n  value_in_range &lt;- \n    values &gt;= min_value & \n    values &lt;= max_value\n  \n  return(DF[value_in_range, ])\n  \n}\n\n\n\n\n\n\n\nNote\n\n\n\nWriting good comments can be challenging, so you better do it in 3 steps:\n\nWrite down whatever comment is on your mind\nRead the comment and see what needs to be improved\nMake the needed improvements"
  },
  {
    "objectID": "posts/01-readiable-code/main.html#code-style",
    "href": "posts/01-readiable-code/main.html#code-style",
    "title": "The Art of Readiable R code",
    "section": "2.3 Code style",
    "text": "2.3 Code style\nIt is important to apply a coding style that make easy to scan the code before going into detail to certain parts. Some advice to improve code style are:\n\nSimilar code should look similar and be grouped in blocks, it will facilitate finding spelling mistakes and prevent repetitive comments.\n\nWe can see how this tips was applied in the keep_rows_in_percentile_range function.\n\nmin_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\nmax_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n\n\nAvoid keeping temporal variables in the global environment .GlobalEnv, instead create a function to make clear the purpose or use pipes (base::|&gt; or magrittr::%&gt;%).\n\nTo ensure this, we created our custom function.\n\nmtcars_renamed |&gt;\n  keep_rows_in_percentile_range(var_name = \"miles_per_gallon\", \n                                min_prob = 0.20,\n                                max_prob = 0.50) |&gt;\n  nrow()\n\n[1] 11\n\n\n\nAvoid writing nested if statements by negating each Boolean test.\n\nIf we hadn’t taken that into consideration our function would be much harder to read.\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n  \n  if(is.data.frame(DF)){\n    \n    values &lt;- DF[[var_name]]\n    \n    if(is.numeric(values)){\n      \n      min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n      max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n      \n      value_in_range &lt;- \n        values &gt;= min_value & \n        values &lt;= max_value\n      \n      return(DF[value_in_range, ])\n      \n    }else{\n      \n      stop(\"var_name should be a numeric column of DF\")\n      \n    }\n\n  }else{\n    \n    stop(\"DF should be a data.frame\")\n    \n  }\n  \n}\n\n\nBoolean validations should be stored in variables should be stored variables to make explicit what was the test about.\nAlways write constants on the right side of the comparison.\nSimplify Boolean comparisons De Morgan’s Law: ! ( A | B) == !A & !B\n\nIn our custom function min_value and max_value works like constants in comparisons to values\n\nvalue_in_range &lt;- \n  values &gt;= min_value & \n  values &lt;= max_value"
  }
]