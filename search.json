[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angel Feliz",
    "section": "",
    "text": "I’m a Data Scientist specializing in R programming with experience in reporting automation, data visualization, and package development. Currently working remotely as a Leasing Reporting Manager at Phoenix Tower International, where I lead development of R packages to improve critical business reports and data workflows.\n\n\nStarting as a junior data analyst, I’ve grown into a data professional who excels at automating processes, ensuring data quality, and delivering actionable insights. My experience spans financial reporting, data migration, and building validation frameworks that have significantly reduced manual effort while improving accuracy.\nI’m passionate about open source contribution and have authored several R packages, including:\n\nbiblegatewayr: For scraping Bible verses\ntidyvalidate: For identifying business errors in tables\ncorrcat: For exploring correlations between categorical variables\n\nI’ve also contributed to the {data.table} package, writing the primary documentation for join operations.\n\n\n\nI’m currently working on a data science portfolio project focused on increasing NYC taxi drivers’ earnings through data-driven strategies. Using the CRISP-DM methodology, I’m analyzing rideshare data from Juno, Uber, Via and Lyft to determine:\n\nHow drivers can increase monthly earnings by selectively skipping trips?\nHow changing initial locations and timing can optimize earnings?\n\nThis project showcases my skills with the R ecosystem, particularly with:\n\nTidyverse packages for data manipulation and visualization\nGeospatial analysis using sf, leaflet, and tmap\nStatistical modeling and inference\nDatabase connectivity and high-performance data manipulation\n\n\n\n\nI hold a Bachelor’s in Industrial Engineering from Instituto Tecnológico de Santo Domingo and am fluent in both English and Spanish. I’m certified in data science, R programming, and data analysis through DataCamp."
  },
  {
    "objectID": "index.html#professional-journey",
    "href": "index.html#professional-journey",
    "title": "Angel Feliz",
    "section": "",
    "text": "Starting as a junior data analyst, I’ve grown into a data professional who excels at automating processes, ensuring data quality, and delivering actionable insights. My experience spans financial reporting, data migration, and building validation frameworks that have significantly reduced manual effort while improving accuracy.\nI’m passionate about open source contribution and have authored several R packages, including:\n\nbiblegatewayr: For scraping Bible verses\ntidyvalidate: For identifying business errors in tables\ncorrcat: For exploring correlations between categorical variables\n\nI’ve also contributed to the {data.table} package, writing the primary documentation for join operations."
  },
  {
    "objectID": "index.html#current-project",
    "href": "index.html#current-project",
    "title": "Angel Feliz",
    "section": "",
    "text": "I’m currently working on a data science portfolio project focused on increasing NYC taxi drivers’ earnings through data-driven strategies. Using the CRISP-DM methodology, I’m analyzing rideshare data from Juno, Uber, Via and Lyft to determine:\n\nHow drivers can increase monthly earnings by selectively skipping trips?\nHow changing initial locations and timing can optimize earnings?\n\nThis project showcases my skills with the R ecosystem, particularly with:\n\nTidyverse packages for data manipulation and visualization\nGeospatial analysis using sf, leaflet, and tmap\nStatistical modeling and inference\nDatabase connectivity and high-performance data manipulation"
  },
  {
    "objectID": "index.html#skills-education",
    "href": "index.html#skills-education",
    "title": "Angel Feliz",
    "section": "",
    "text": "I hold a Bachelor’s in Industrial Engineering from Instituto Tecnológico de Santo Domingo and am fluent in both English and Spanish. I’m certified in data science, R programming, and data analysis through DataCamp."
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "",
    "text": "Data preprocessing is the cornerstone of robust machine learning pipelines. Drawing from Max Kuhn’s Applied Predictive Modeling, this guide explores essential transformations through the {tidymodels} lens. We’ll bridge theory with practical implementation, examining when, why, and how to apply each technique while weighing their tradeoffs."
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#centering-and-scaling",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#centering-and-scaling",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "1.1 Centering and Scaling",
    "text": "1.1 Centering and Scaling\nWhen to Use:\n\nModels sensitive to predictor magnitude (SVM, KNN, neural networks)\nBefore dimensionality reduction (PCA) or spatial sign transformations\nWhen predictors have different measurement scales\n\nWhy It Matters:\n\nCenters variables around zero (μ=0)\nStandardizes variance (σ=1)\nEnables meaningful coefficient comparisons\nCritical for distance-based calculations and numerical stability\n\nImplementation:\n\nlibrary(tidymodels)\n\nnorm_recipe &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  prep()\n\nbake(norm_recipe, new_data = NULL) |&gt; summary()\n\n      cyl              disp               hp               drat        \n Min.   :-1.225   Min.   :-1.2879   Min.   :-1.3810   Min.   :-1.5646  \n 1st Qu.:-1.225   1st Qu.:-0.8867   1st Qu.:-0.7320   1st Qu.:-0.9661  \n Median :-0.105   Median :-0.2777   Median :-0.3455   Median : 0.1841  \n Mean   : 0.000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 1.015   3rd Qu.: 0.7688   3rd Qu.: 0.4859   3rd Qu.: 0.6049  \n Max.   : 1.015   Max.   : 1.9468   Max.   : 2.7466   Max.   : 2.4939  \n       wt               qsec                vs               am         \n Min.   :-1.7418   Min.   :-1.87401   Min.   :-0.868   Min.   :-0.8141  \n 1st Qu.:-0.6500   1st Qu.:-0.53513   1st Qu.:-0.868   1st Qu.:-0.8141  \n Median : 0.1101   Median :-0.07765   Median :-0.868   Median :-0.8141  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.000   Mean   : 0.0000  \n 3rd Qu.: 0.4014   3rd Qu.: 0.58830   3rd Qu.: 1.116   3rd Qu.: 1.1899  \n Max.   : 2.2553   Max.   : 2.82675   Max.   : 1.116   Max.   : 1.1899  \n      gear              carb              mpg       \n Min.   :-0.9318   Min.   :-1.1222   Min.   :10.40  \n 1st Qu.:-0.9318   1st Qu.:-0.5030   1st Qu.:15.43  \n Median : 0.4236   Median :-0.5030   Median :19.20  \n Mean   : 0.0000   Mean   : 0.0000   Mean   :20.09  \n 3rd Qu.: 0.4236   3rd Qu.: 0.7352   3rd Qu.:22.80  \n Max.   : 1.7789   Max.   : 3.2117   Max.   :33.90  \n\n\nPros:\n\nRequired for distance-based algorithms\nImproves numerical stability\nFacilitates convergence in gradient-based methods\n\nCons:\n\nLoses original measurement context\nNot needed for tree-based models\nSensitive to outlier influence\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways calculate scaling parameters from training data only to avoid data leakage. Resampling should encapsulate preprocessing steps for honest performance estimation."
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#resolving-skewness",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#resolving-skewness",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "1.2 Resolving Skewness",
    "text": "1.2 Resolving Skewness\nWhen to Use:\n\nSmallest to largest ratio &gt; 20 (max/min)\nRight/left-tailed distributions (|skewness| &gt; 1)\nBefore linear model assumptions\nWhen preparing for PCA or other variance-sensitive methods\n\nSkewness Formula:\n\\[skewness = \\frac{\\sum(x_i - \\bar{x})^3}{(n-1)v^{3/2}} \\quad \\text{where } v = \\frac{\\sum(x_i - \\bar{x})^2}{(n-1)}\\]\nBox-Cox Implementation:\n\ndata(ames, package = \"modeldata\")\n\nskew_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area, data = ames) |&gt;\n  step_BoxCox(Gr_Liv_Area, limits = c(-2, 2)) |&gt; # MLE for λ\n  prep()\n\ntidy(skew_recipe) # Shows selected λ value\n\n# A tibble: 1 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      BoxCox TRUE    FALSE BoxCox_QjqKE\n\n# Calculate original skewness\names |&gt; \n  summarize(skewness = moments::skewness(Gr_Liv_Area))\n\n# A tibble: 1 × 1\n  skewness\n     &lt;dbl&gt;\n1     1.27\n\n\nTransformation Options:\n\nλ=2 → Square\nλ=0.5 → Square root\nλ=-1 → Inverse\nλ=0 → Natural log\n\nPros:\n\nData-driven transformation selection\nHandles zero values gracefully\nContinuous transformation spectrum\n\nCons:\n\nRequires strictly positive values\nLoses interpretability\nSensitive to outlier influence"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#spatial-sign-for-outliers",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#spatial-sign-for-outliers",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "2.1 Spatial Sign for Outliers",
    "text": "2.1 Spatial Sign for Outliers\nWhen to Use:\n\nHigh-dimensional data\nModels sensitive to outlier magnitude (linear regression)\nWhen robust scaling isn’t sufficient\nDealing with radial outliers in multidimensional space\n\nCritical Considerations:\n\nInvestigate outliers for data entry errors first\nConsider cluster validity before removal\nUnderstand missingness mechanism (MCAR/MAR/MNAR)\n\nImplementation:\n\noutlier_recipe &lt;- recipe(Species ~ Sepal.Length + Sepal.Width, data = iris) |&gt;\n  step_normalize(all_numeric()) |&gt; # Mandatory first step\n  step_spatialsign(all_numeric()) |&gt;\n  prep()\n\nbake(outlier_recipe, new_data = NULL) |&gt;\n  ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point()+\n  theme_minimal()\n\n\n\n\n\n\n\n\nPros:\n\nRobust to extreme outliers\nMaintains relative angles\nNon-parametric approach\n\nCons:\n\nDestroys magnitude information\nRequires centered/scaled data\nNot suitable for sparse data"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#pca-for-data-reduction",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#pca-for-data-reduction",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "2.2 PCA for Data Reduction",
    "text": "2.2 PCA for Data Reduction\nOptimal Workflow:\n\nResolve skewness (Box-Cox/Yeo-Johnson)\nCenter/scale predictors\nDetermine components via cross-validation/scree plot\nValidate via resampling\n\nComponent Selection:\n\nRetain components before scree plot elbow\nCumulative variance &gt;80-90%\nCross-validate performance\n\nImplementation:\n\npca_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric(), num_comp = 4L) |&gt; #tune()\n  prep()\n\n# Scree plot visualization\npca_vars &lt;- tidy(pca_recipe, 2, type = \"variance\")\n\npca_vars |&gt; \n  filter(terms == \"percent variance\") |&gt;\n  ggplot(aes(component, value)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Scree Plot\", y = \"% Variance Explained\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Component interpretation\ntidy(pca_recipe, 2) |&gt; \n  filter(component == \"PC1\") |&gt; \n  arrange(-abs(value))\n\n# A tibble: 4 × 4\n  terms         value component id       \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 Petal.Length  0.580 PC1       pca_cE6O1\n2 Petal.Width   0.565 PC1       pca_cE6O1\n3 Sepal.Length  0.521 PC1       pca_cE6O1\n4 Sepal.Width  -0.269 PC1       pca_cE6O1\n\n\nPros:\n\nRemoves multicollinearity\nReduces computational load\nReveals latent structure\n\nCons:\n\nLoss of interpretability\nSensitive to scaling\nLinear assumptions\nSupervised methods (PLS) may be preferable for outcome-aware reduction"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#missing-value-imputation",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#missing-value-imputation",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "3.1 Missing Value Imputation",
    "text": "3.1 Missing Value Imputation\nCritical Considerations:\n\nInformative missingness: Is missing pattern related to outcome?\nCensored data: Different treatment than MCAR/MAR\n\n5% missing → Consider removal\n\nType-appropriate methods (KNN vs regression)\n\nImputation Strategies:\n\n\n\nScenario\nApproach\n\n\n\n\n&lt;5% missing\nMedian/mode imputation\n\n\nContinuous predictors\nKNN, linear regression, bagging\n\n\nCategorical\nMode, multinomial logit\n\n\nHigh dimensionality\nRegularized models, MICE\n\n\n\nImplementation:\n\names2 &lt;- ames\names2$Year_Built2 &lt;- ames2$Year_Built\n\nset.seed(5858)\names2[sample.int(2930, 1000), c(\"Year_Built2\")] &lt;- NA_integer_\names2[sample.int(2930, 800), c(\"Lot_Frontage\")] &lt;- NA_integer_\n\nimpute_recipe &lt;- recipe(Sale_Price ~ Lot_Frontage + Year_Built2 + Year_Built, data = ames2) |&gt;\n  step_impute_knn(Lot_Frontage, neighbors = 3L) |&gt; #tune()\n  step_impute_linear(Year_Built2, impute_with = imp_vars(Year_Built)) |&gt;\n  prep()\n\n# Assess imputation quality\ncomplete_data &lt;- bake(impute_recipe, new_data = ames2)\ncor(complete_data$Year_Built, complete_data$Year_Built2, use = \"complete.obs\")\n\n[1] 1\n\ncor(complete_data$Lot_Frontage, ames$Lot_Frontage, use = \"complete.obs\")\n\n[1] 0.8296254"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#feature-filtering",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#feature-filtering",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "3.2 Feature Filtering",
    "text": "3.2 Feature Filtering\nNear-Zero Variance Detection:\n\nFrequency ratio &gt; 20\nUnique values &lt; 10%\nPercent unique = n_unique/n * 100\n\n\nnzv_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_nzv(all_predictors(), freq_cut = 95/5, unique_cut = 10) |&gt;\n  prep()\n\ntidy(nzv_recipe)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id       \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;    \n1      1 step      nzv   TRUE    FALSE nzv_sEXIo\n\n\nMulticollinearity Handling:\n\nVariance Inflation Factor (VIF) &gt; 5-10\nPairwise correlation threshold\nIterative removal algorithm\n\n\ncorr_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_corr(all_numeric(), threshold = 0.9, method = \"spearman\") |&gt;\n  prep()\n\ntidy(corr_recipe)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id        \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;     \n1      1 step      corr  TRUE    FALSE corr_aKa2d"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#categorical-encoding-nonlinear-terms",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#categorical-encoding-nonlinear-terms",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "4.1 Categorical Encoding & Nonlinear Terms",
    "text": "4.1 Categorical Encoding & Nonlinear Terms\nBest Practices:\n\nDummy variables for nominal predictors (one-hot encoding)\nOrdered factors for ordinal categories\nInclude interaction terms where domain knowledge suggests\nAdd polynomial terms for known nonlinear relationships\n\nExample:\n\nnonlinear_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_dummy(all_nominal(), -all_outcomes()) |&gt;\n  step_poly(Sepal.Length, degree = 2) |&gt;\n  step_interact(~ Sepal.Width:Petal.Length) |&gt;\n  prep()"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#distance-to-class-centroids",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#distance-to-class-centroids",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "4.2 Distance to Class Centroids",
    "text": "4.2 Distance to Class Centroids\nWhen to Use:\n\nClassification problems\nCluster-aware feature engineering\nImproving linear separability\nAugmenting existing feature set\n\nImplementation:\n\ncentroid_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_classdist(all_numeric(), class = \"Species\", pool = FALSE) |&gt;\n  prep()\n\nbake(centroid_recipe, new_data = NULL) |&gt;\n  select(starts_with(\"classdist_\")) |&gt;\n  head()\n\n# A tibble: 6 × 3\n  classdist_setosa classdist_versicolor classdist_virginica\n             &lt;dbl&gt;                &lt;dbl&gt;               &lt;dbl&gt;\n1           -0.800                 4.74                5.21\n2            0.733                 4.42                5.04\n3            0.250                 4.55                5.08\n4            0.534                 4.42                4.95\n5           -0.272                 4.79                5.22\n6            1.31                  4.79                5.21"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#binning-strategies",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#binning-strategies",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "4.3 Binning Strategies",
    "text": "4.3 Binning Strategies\nWhen to Avoid:\n\nManual binning pre-analysis\nWith tree-based models\nSmall sample sizes\nWhen interpretability trumps accuracy\n\nEthical Considerations:\n\nMedical diagnostics require maximum accuracy\nLegal implications of arbitrary thresholds\nPotential bias introduction through careless discretization\n\nSmart Discretization:\n\nbin_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area, data = ames) |&gt;\n  step_discretize(Gr_Liv_Area, num_breaks = 4, min_unique = 10) |&gt;\n  prep()\n\nbake(bin_recipe, new_data = NULL) |&gt;\n  count(Gr_Liv_Area)\n\n# A tibble: 4 × 2\n  Gr_Liv_Area     n\n  &lt;fct&gt;       &lt;int&gt;\n1 bin1          735\n2 bin2          733\n3 bin3          729\n4 bin4          733"
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html",
    "href": "posts/02-imputing-missing-values/main.html",
    "title": "Handling Missing Values",
    "section": "",
    "text": "If you are working with real data, it’s normal to find missing values, so it’s very important to understand how to manage them correctly. In R, the default for many functions is to remove missing values to create plots or train machine learning models, but that can be very dangerous as we are adding bias to our analysis that could compromise our final conclusions.\nBut removing values could be a good option if we meet the following statements:\n\nOnly 5% of your dataset’s rows present missing values.\nAll the values have the same probability to be missing. That’s when we say that they are missing completely at random (MCAR).\n\nOtherwise, the best way to handle missing values is to impute values based on general patterns found in the data. If we cannot find patterns in the current data, we will need to find more data until finding a valid pattern to impute the values."
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#introduction",
    "href": "posts/02-imputing-missing-values/main.html#introduction",
    "title": "Handling Missing Values",
    "section": "",
    "text": "If you are working with real data, it’s normal to find missing values, so it’s very important to understand how to manage them correctly. In R, the default for many functions is to remove missing values to create plots or train machine learning models, but that can be very dangerous as we are adding bias to our analysis that could compromise our final conclusions.\nBut removing values could be a good option if we meet the following statements:\n\nOnly 5% of your dataset’s rows present missing values.\nAll the values have the same probability to be missing. That’s when we say that they are missing completely at random (MCAR).\n\nOtherwise, the best way to handle missing values is to impute values based on general patterns found in the data. If we cannot find patterns in the current data, we will need to find more data until finding a valid pattern to impute the values."
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#imputation-practical-example",
    "href": "posts/02-imputing-missing-values/main.html#imputation-practical-example",
    "title": "Handling Missing Values",
    "section": "2 Imputation practical example",
    "text": "2 Imputation practical example\nLet’s assume that all the predictors for a machine learning model are stored in the datasets::airquality data frame, which has some missing values, so we need to explore and decide what to do in this case.\nTo perform all the tasks needed to solve the missing values problems in our dataset, we will load the following packages:\n\n# For modeling and statistical analysis\nlibrary(tidymodels)\n\n# For modeling lasso or elastic-net regression\nlibrary(glmnet)\n\n# For exploring missing values\nlibrary(naniar)\nlibrary(mice)\n\n\n2.1 Confirming if values are MCAR\nBased on the next test, we can reject the null hypothesis and conclude that the missing values aren’t completely at random, so we will need to impute the missing values.\n\nmcar_test(airquality)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      35.1    14 0.00142                4\n\n\n\n\n2.2 Explore missing values patterns\nOnce we know that we need to impute values, it’s important to know that the column with more missing values is the Ozone with 37 values to impute, which we can divide between the ones that can use the rest of the features (35 rows) and the ones that can use all the columns except the Solar.R as they are missing (2 rows).\n\nairquality |&gt;\n  md.pattern() |&gt;\n  invisible()\n\n\n\n\n\n\n\n\n\n\n2.3 Imputing Ozone values\n\n2.3.1 Exploring missing values\nIn the next plot we can see how the missing Ozone values are spread across a wide range of values, so we wouldn’t be able to find big differences between the means of columns with or without missing Ozone values.\nAs we are plotting all variables against the target variable, it’s easy to see that the Temp, Wind and Solar.R present a not linear relation with Ozone and we cannot see a clear pattern for Day and Month.\n\nairquality |&gt;\n  pivot_longer(cols = -Ozone) |&gt;\n  ggplot(aes(value, Ozone))+\n  geom_miss_point(aes(color = is.na(Ozone)),\n                  alpha = 0.5,\n                  show.legend = FALSE)+\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"black\")+\n  scale_color_manual(values = c(\"TRUE\" = \"red\",\n                                \"FALSE\" = \"gray60\"))+\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_light()\n\n\n\n\n\n\n\n\nBased on this result, we know it’s necessary to train a model able to catch the non-linear patterns and omit the Day and Month as they don’t show any relation with Ozone.\n\n\n2.3.2 Training the model to impute\nAs we need to create 2 very similar models (one using the Solar.R column and other without it) it’s better to create a function.\n\nfit_tuned_regression_model &lt;- function(df,\n                                       model_to_tune,\n                                       model_grid,\n                                       formula,\n                                       step_fun,\n                                       seed = 1234){\n  \n  # Defining empty model workflow\n  y_wf &lt;- \n    recipe(formula, data = df) |&gt;\n    step_fun() |&gt;\n    workflow(model_to_tune)\n  \n  # Defining re-samples based on seed\n  set.seed(seed)\n  y_resamples &lt;- mc_cv(df, times = 30)\n  set.seed(NULL)\n  \n  # Fitting re-sample for each grid level\n  y_tuned &lt;- tune_grid(\n    y_wf,\n    resamples = y_resamples,\n    grid = model_grid,\n    metrics = metric_set(yardstick::rsq)\n  )\n\n  # Selecting the best model\n  y_trained_model &lt;- \n    finalize_workflow(y_wf,\n                      select_best(y_tuned)) |&gt;\n    fit(df)\n  \n  # Presenting results\n  results &lt;- list(\"fit\" = y_trained_model,\n                  \"best_fit\" = show_best(y_tuned))\n  \n  print(results$best_fit)\n  \n  return(results)\n  \n}\n\nThen we need to define common inputs for both models.\n\nGlmModel &lt;- linear_reg(\n  engine = \"glmnet\",\n  penalty = tune(),\n  mixture = tune()\n)\n\nGlmGrid &lt;- grid_regular(\n  penalty(),\n  mixture(),\n  levels = 10\n)\n\nozone_steps &lt;- function(recipe){\n  \n  recipe |&gt;\n  step_poly(all_numeric_predictors(),\n            degree = 2) |&gt;\n    step_interact(terms = ~(. -Ozone)^2) |&gt;\n    step_zv(all_numeric_predictors()) |&gt;\n    step_scale(all_numeric_predictors())\n  \n}\n\nNow we can fit each model.\n\nOzoneSolarGlmFitted &lt;- fit_tuned_regression_model(\n  df = na.omit(airquality),\n  model_to_tune = GlmModel,\n  model_grid = GlmGrid,\n  formula = as.formula(\"Ozone ~ Solar.R + Temp + Wind\"),\n  step_fun = ozone_steps,\n  seed = 5050\n)\n\nWarning in select_best(y_tuned): No value of `metric` was given; \"rsq\" will be\nused.\n\n\nWarning in show_best(y_tuned): No value of `metric` was given; \"rsq\" will be\nused.\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 1               1     rsq     standard   0.721    30  0.0159 Preprocessor1_Mo…\n2 1               0.889 rsq     standard   0.721    30  0.0157 Preprocessor1_Mo…\n3 1               0.778 rsq     standard   0.720    30  0.0156 Preprocessor1_Mo…\n4 0.0000000001    0     rsq     standard   0.718    30  0.0165 Preprocessor1_Mo…\n5 0.00000000129   0     rsq     standard   0.718    30  0.0165 Preprocessor1_Mo…\n\nOzoneNotSolarGlmFitted &lt;- airquality |&gt;\n  select(-Solar.R) |&gt;\n  na.omit() |&gt;\n  fit_tuned_regression_model(model_to_tune = GlmModel,\n                         model_grid = GlmGrid,\n                         formula = as.formula(\"Ozone ~ Temp + Wind\"),\n                         step_fun = ozone_steps,\n                         seed = 4518)\n\nWarning in select_best(y_tuned): No value of `metric` was given; \"rsq\" will be used.\nNo value of `metric` was given; \"rsq\" will be used.\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001        0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n2 0.00000000129       0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n3 0.0000000167        0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n4 0.000000215         0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n5 0.00000278          0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n\n\n\n\n2.3.3 Impute missing values\nOnce we have both models, we can impute the Ozone values, but let’s also create a function to perform this task as we will need to repeat the process very time we need to predict a new value.\n\nimpute_ozone &lt;- function(df,\n                         solar_model,\n                         no_solar_model){\n  \n  mutate(df,\n         Ozone_NA = is.na(Ozone),\n         Ozone = case_when(\n           !is.na(Ozone) ~ Ozone,\n           !is.na(Solar.R)~\n             predict(solar_model, new_data = df)$.pred,\n           TRUE ~\n             predict(no_solar_model, new_data = df)$.pred)\n  )\n}\n\n\nAirOzoneImputed &lt;- impute_ozone(\n  airquality,\n  solar_model = OzoneSolarGlmFitted$fit,\n  no_solar_model = OzoneNotSolarGlmFitted$fit\n)\n\nBy plotting the imputed Ozone values can see that the values follow the patterns present in the non-missing values.\n\nAirOzoneImputed |&gt;\n  pivot_longer(cols = -c(Ozone, Ozone_NA)) |&gt;\n  na.omit() |&gt;\n  ggplot(aes(value, Ozone, color = Ozone_NA))+\n  geom_point(show.legend = FALSE)+\n  scale_color_manual(values = c(\"TRUE\" = \"red\",\n                                \"FALSE\" = \"grey60\"))+\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n\n2.4 Fixing Solar.R values\nOnce we don’t have any missing value in the Ozone column we can explore the remaining 7 missing values in the Solar.R column.\n\nAirOzoneImputed |&gt;\n  md.pattern() |&gt;\n  invisible()\n\n\n\n\n\n\n\n\nNow the missing values represent only 5% of the rows.\n\nnrow(na.omit(AirOzoneImputed)) / nrow(AirOzoneImputed) * 100\n\n[1] 95.42484\n\n\nAnd we don’t have enough data to reject the null hypothesis, and we can not affirm that the missing values are not missing completely at random.\n\nmcar_test(AirOzoneImputed)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      10.3     6   0.114                2\n\n\nSo we can remove the remaining missing values.\n\nAirqualityImputed &lt;- na.omit(AirOzoneImputed)\n\nhead(AirqualityImputed)\n\n  Ozone Solar.R Wind Temp Month Day Ozone_NA\n1    41     190  7.4   67     5   1    FALSE\n2    36     118  8.0   72     5   2    FALSE\n3    12     149 12.6   74     5   3    FALSE\n4    18     313 11.5   62     5   4    FALSE\n7    23     299  8.6   65     5   7    FALSE\n8    19      99 13.8   59     5   8    FALSE"
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#final-thoughts",
    "href": "posts/02-imputing-missing-values/main.html#final-thoughts",
    "title": "Handling Missing Values",
    "section": "3 Final thoughts",
    "text": "3 Final thoughts\nI hope this blog can help to improve your ability to handle missing values without compromising your results.\nDon’t forget to explore well your columns as missing values are usually encoded with 0 or 99."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html",
    "href": "posts/03-intro-to-rocker/main.html",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "",
    "text": "El lenguaje R es ampliamente reconocido por su versatilidad en el análisis de datos, y aunque se puede instalar y ejecutar directamente R y RStudio en casi cualquier sistema, usar entornos basados en contenedores —como los que ofrece el Rocker Project— nos brinda ventajas significativas a largo plazo."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#r-una-herramienta-excepcional-para-el-análisis-de-datos",
    "href": "posts/03-intro-to-rocker/main.html#r-una-herramienta-excepcional-para-el-análisis-de-datos",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "1 R: Una herramienta excepcional para el análisis de datos",
    "text": "1 R: Una herramienta excepcional para el análisis de datos\nR destaca por sus numerosas cualidades que lo hacen ideal para científicos de datos y analistas:\n\nGratuito y de código abierto: Permite el acceso libre a un ecosistema de paquetes y herramientas.\nAmplia variedad de herramientas: Existen innumerables paquetes y librerías diseñados para tareas específicas de análisis, estadística y visualización.\nFácil de aprender: Su sintaxis y comunidad de apoyo facilitan el proceso de aprendizaje para nuevos usuarios.\nMultiplataforma: Se ejecuta sin problemas en Windows, macOS y Linux.\nComunidad activa: La colaboración y el constante desarrollo impulsan la innovación y mejora continua del entorno."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#la-importancia-de-la-reproducibilidad-a-lo-largo-del-tiempo",
    "href": "posts/03-intro-to-rocker/main.html#la-importancia-de-la-reproducibilidad-a-lo-largo-del-tiempo",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "2 La importancia de la reproducibilidad a lo largo del tiempo",
    "text": "2 La importancia de la reproducibilidad a lo largo del tiempo\nCuando desarrollamos un programa en R, es fundamental que pueda ser reproducido incluso años después. Para lograrlo, es necesario garantizar:\n\nConsistencia del entorno: Utilizar la misma versión del sistema operativo, de R y de los paquetes instalados.\nPrevención de incompatibilidades: Las actualizaciones y cambios en el software pueden alterar el comportamiento del código.\nVerificación futura: Asegurar que los análisis y resultados se puedan validar y comparar con versiones anteriores.\nIntegridad en la investigación: Una base reproducible es esencial para la honestidad y la claridad en la comunicación científica."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#ventajas-de-usar-entornos-de-desarrollo-en-contenedores",
    "href": "posts/03-intro-to-rocker/main.html#ventajas-de-usar-entornos-de-desarrollo-en-contenedores",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "3 Ventajas de usar entornos de desarrollo en contenedores",
    "text": "3 Ventajas de usar entornos de desarrollo en contenedores\nEl uso de contenedores aporta un gran beneficio para la gestión y evolución de nuestros proyectos:\n\nAislamiento: El entorno de ejecución se mantiene separado del sistema operativo del host, evitando conflictos de dependencias.\nPortabilidad: Un contenedor puede ser ejecutado en cualquier máquina que tenga Docker, sin necesidad de reconfigurar el entorno.\nEscalabilidad: Es sencillo trasladar el entorno local a servidores de mayor capacidad cuando el proyecto crece.\nSeguridad y consistencia: Garantiza que el código se ejecute de manera idéntica, reduciendo errores inesperados y facilitando la colaboración.\nMantenimiento simplificado: Las actualizaciones y cambios se gestionan centralizadamente sin afectar el entorno de producción."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#rocker-simplificando-el-desarrollo-reproducible",
    "href": "posts/03-intro-to-rocker/main.html#rocker-simplificando-el-desarrollo-reproducible",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "4 Rocker: Simplificando el desarrollo reproducible",
    "text": "4 Rocker: Simplificando el desarrollo reproducible\nEl Rocker Project ha hecho el trabajo pesado al proporcionar imágenes Docker preconfiguradas que incluyen R, RStudio y otros entornos especializados.\nPara obtener todos estos beneficios solo tenemos que seguir los siguientes pasos:\n\nInstalar Docker en tu sistema operativo.\n\nSi usas Windows o Mac puedes descargar Docker Desktop desde la página oficial.\nSi usas Linux correr el siguiente bash script con permisos de administrador.\n\n\napt update\nsudo apt install apt-transport-https ca-certificates curl gnupg2 software-properties-common\ncurl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -\nadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/debian\" bookworm stable\napt update\napt-cache policy docker-ce\napt install docker-ce\nsystemctl status docker\n/usr/sbin/usermod -aG docker &lt;YOUR-USER&gt;\nsu - &lt;YOUR-USER&gt;\n\nCrea una carpeta en la que almacenar todos los proyectos de R &lt;mis-proyectos&gt;.\nDentro de &lt;mis-proyectos&gt; crea otra carpeta para almacenar copias de las librerías que irás usando en distintos proyectos &lt;mis-paquetes&gt;.\nDescarga y activa el contenedor a usar mediante el siguiente comando dentro de la línea de comando o dentro de Docker Desktop.\n\ndocker run --rm -tid --name rstudio \\\n  -e DISABLE_AUTH=true \\\n  -p 8787:8787 \\\n  -v &lt;mis-proyectos&gt;:/home/rstudio \\\n  -v &lt;mis-proyectos&gt;/&lt;mis-paquetes&gt;:/opt/r-cache \\\n  ghcr.io/rocker-org/geospatial:4.4.3\n\nAbre una ventana en tu navegador y ve al link http://localhost:8787/ y verás a RStudio funcionando desde el contenedor."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#configurando-git-y-github",
    "href": "posts/03-intro-to-rocker/main.html#configurando-git-y-github",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "5 Configurando git y github",
    "text": "5 Configurando git y github\nUna vez ya tienes R y RStudio funcionando ya podrás crear tus primeros proyectos y querrás compartir tu progreso con otros.\nAsí como usamos las redes sociales para compartir nuestras experiencias, los programadores tienen una red social llamada GitHub en la que compartimos nuestro código y nos ayudamos mutuamente.\nComo ya el contenedor que estás corriendo tiene instalado git solo necesitas seguir los siguientes pasos para comenzar a utilizar esta poderosa herramienta.\n\nCrea una cuenta en github.com.\nDa clic al ícono con tu cuenta y da clic en Settings.\nAccede a SSH and GPG keys.\nEn la esquina superior derecha dale clic a New SSH key.\nDefine un nombre para tu clave como puede ser &lt;primera-clave&gt;.\nEn RStudio ve a la terminal y ejecuta el comando ssh-keygen y luego da enter para generar una clave pública y una clave privada.\nCopia y pega el contenido de tu clave pública ubicada en la siguiente dirección dentro de la caja de Key:\n\nDesde RStudio: /home/rstudio/.ssh/id_rsa.pub\nDesde tu PC: /.ssh/id_rsa.pub\n\nDa clic en el botón Add SSH key.\nAhora solo queda configurar git con la misma cuenta que tenemos creada en GitHub al correr los siguientes comandos.\n\ngit config --global user.name \"Nombre Apellido\"\ngit config --global user.email \"user@email.com\""
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#stopping-container",
    "href": "posts/03-intro-to-rocker/main.html#stopping-container",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "6 Stopping container",
    "text": "6 Stopping container\nOnce you have completed your task we can stop the container by running.\ndocker stop rstudio"
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html",
    "href": "posts/05-preprocessing-by-classification-model/main.html",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "",
    "text": "Preprocessing is a critical step in building effective machine learning models. It involves transforming, encoding, scaling, imputing, and selecting features to optimize model performance. These steps directly influence generalization, training speed, regularization, and interpretability. The choice of preprocessing techniques, such as scaling, normalization, or encoding, can significantly impact classification metrics, with different algorithms responding uniquely to the same pipeline.\nKey principles for a reproducible preprocessing pipeline include:\n\nPreprocessing is part of the model. Use the recipes and workflows packages in R to ensure transformations are trained on the training set and consistently applied to new data, preventing data leakage or mismatched transformations during prediction.\nDocument the purpose of each step. Group preprocessing steps by the issues they address (e.g., skewness, scale, missingness, collinearity) and explain why specific model families require them.\n\nThis guide provides a comprehensive overview of preprocessing with the recipes package, tailored to different model families, along with a practical baseline recipe and checklist."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#tree-based-and-rule-based-classification",
    "href": "posts/05-preprocessing-by-classification-model/main.html#tree-based-and-rule-based-classification",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.1 Tree-Based and Rule-Based Classification",
    "text": "3.1 Tree-Based and Rule-Based Classification\nBehavior: Tree-based models (e.g., ranger, xgboost, rpart, C5_rules, bart, bag_tree, boost_tree, rand_forest, rule_fit) rely on splits based on feature ordering and thresholds, making them robust to monotonic transformations and less sensitive to feature scale. However, they benefit from explicit date features and proper handling of categorical variables.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data, as some tree-based engines (e.g., xgboost) can handle missing values internally, but imputation is safer for consistency.\nExtract date features (step_date() / step_holiday()): Creates features like day of the week or holiday indicators, which trees can use for splits.\nHandle rare categories (step_other() / step_novel()): Collapses infrequent categorical levels and prepares for unseen categories during prediction.\nEncode categorical variables (step_dummy()): Converts categories to numeric indicators, required for engines like xgboost, C5.0, or lightgbm, but optional for ranger, rpart, or partykit.\nScale features (step_normalize()): Only necessary for RuleFit (xrf) or similar hybrid models that combine trees with linear components.\nDiscretize continuous variables (step_discretize()): Optional, but can simplify relationships for certain tree-based models like C5.0 or rpart."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#linear-and-generalized-linear-classification-models",
    "href": "posts/05-preprocessing-by-classification-model/main.html#linear-and-generalized-linear-classification-models",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.2 Linear and Generalized Linear Classification Models",
    "text": "3.2 Linear and Generalized Linear Classification Models\nBehavior: Linear models (e.g., glm, glmnet, multinom_reg, logistic_reg), generalized additive models (gen_additive_mod), and multivariate adaptive regression splines (mars, bag_mars) are highly sensitive to feature scale and assume linear or smooth relationships. Feature engineering, such as splines and interactions, is critical for capturing complex patterns.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data for model fitting.\nExtract date features (step_date() / step_holiday()): Creates linear predictors from date columns.\nAdd splines or interactions (step_ns() / step_bs() / step_interact()): Captures non-linear and non-additive relationships, boosting performance for glm, glmnet, gen_additive_mod, and mars.\nReduce skewness (step_YeoJohnson()): Normalizes skewed numeric predictors for better model stability.\nEncode categorical variables (step_dummy()): Converts categorical predictors to numeric format for glmnet, LiblineaR, keras, or spark.\nScale features (step_normalize()): Ensures comparable scale for penalized models like glmnet or multinom_reg.\nAddress multicollinearity (step_corr() / step_pca()): Reduces redundancy among correlated predictors.\nCustom transformations (step_mutate()): Allows manual feature engineering, such as creating ratios or log transformations, especially useful for gen_additive_mod or mars."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#kernel-based-models-and-k-nearest-neighbors-svm-knn",
    "href": "posts/05-preprocessing-by-classification-model/main.html#kernel-based-models-and-k-nearest-neighbors-svm-knn",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.3 Kernel-Based Models and k-Nearest Neighbors (SVM, KNN)",
    "text": "3.3 Kernel-Based Models and k-Nearest Neighbors (SVM, KNN)\nBehavior: Kernel-based models (svm_linear, svm_poly, svm_rbf) and KNN (nearest_neighbor) rely on distance calculations, making them highly sensitive to feature scale and outliers.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data for distance calculations.\nExtract date features (step_date() / step_holiday()): Creates numeric features from dates.\nReduce skewness (step_YeoJohnson()): Minimizes the impact of outliers on distance metrics.\nEncode categorical variables (step_dummy()): Converts categorical predictors to numeric format for LiblineaR, kernlab, or liquidSVM.\nScale features (step_normalize() / step_range()): Ensures all features are on the same scale, critical for distance-based models.\nAddress multicollinearity (step_pca()): Reduces dimensionality to improve performance."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#neural-networks-mlps",
    "href": "posts/05-preprocessing-by-classification-model/main.html#neural-networks-mlps",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.4 Neural Networks (MLPs)",
    "text": "3.4 Neural Networks (MLPs)\nBehavior: Neural networks (mlp, bag_mlp) are sensitive to input scale and benefit from normalized, well-distributed features for stable and faster training.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data.\nExtract date features (step_date() / step_holiday()): Creates numeric features from dates.\nReduce skewness (step_YeoJohnson()): Mitigates the impact of skewed distributions and outliers.\nEncode categorical variables (step_dummy()): Converts categorical features to numeric format for brulee, keras, or nnet.\nScale features (step_normalize() / step_range()): Scales inputs to a small range for stable training.\nCustom transformations (step_mutate()): Enables custom feature engineering for complex datasets."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#discriminant-analysis-models",
    "href": "posts/05-preprocessing-by-classification-model/main.html#discriminant-analysis-models",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.5 Discriminant Analysis Models",
    "text": "3.5 Discriminant Analysis Models\nBehavior: Discriminant analysis models, including linear discriminant analysis (discrim_linear, engines: MASS, mda, sda, sparsediscrim), quadratic discriminant analysis (discrim_quad, engines: MASS, sparsediscrim), flexible discriminant analysis (discrim_flexible, earth engine), and regularized discriminant analysis (discrim_regularized, klaR engine), assume multivariate normality (especially LDA and QDA) and are sensitive to feature scale and multicollinearity. They benefit from normalized and decorrelated features.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data for model fitting.\nExtract date features (step_date() / step_holiday()): Creates numeric predictors from dates.\nReduce skewness (step_YeoJohnson()): Normalizes skewed predictors, critical for LDA and QDA, which assume normality.\nEncode categorical variables (step_dummy()): Converts categorical predictors to numeric format, required for most engines.\nScale features (step_normalize()): Ensures comparable scale, essential for discrim_linear and discrim_quad.\nAddress multicollinearity (step_corr() / step_pca()): Reduces redundancy, especially important for LDA and QDA to avoid singularity issues.\nCustom transformations (step_mutate()): Allows manual feature engineering for complex patterns, particularly for discrim_flexible."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#naive-bayes-models",
    "href": "posts/05-preprocessing-by-classification-model/main.html#naive-bayes-models",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.6 Naive Bayes Models",
    "text": "3.6 Naive Bayes Models\nBehavior: Naive Bayes models (naive_Bayes, engines: klaR, naivebayes) assume feature independence and are sensitive to feature distributions (e.g., Gaussian Naive Bayes assumes normality). They handle categorical variables natively but benefit from proper encoding and imputation for numeric-only implementations.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data.\nExtract date features (step_date() / step_holiday()): Creates numeric or categorical features from dates.\nReduce skewness (step_YeoJohnson()): Normalizes skewed numeric predictors for Gaussian Naive Bayes.\nEncode categorical variables (step_dummy()): Required for numeric-only implementations (e.g., klaR with certain settings).\nDiscretize continuous variables (step_discretize()): Optional for non-Gaussian Naive Bayes to simplify continuous feature distributions.\nHandle rare categories (step_other() / step_novel()): Manages infrequent or unseen categorical levels."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#partial-least-squares-pls",
    "href": "posts/05-preprocessing-by-classification-model/main.html#partial-least-squares-pls",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.7 Partial Least Squares (PLS)",
    "text": "3.7 Partial Least Squares (PLS)\nBehavior: Partial least squares (pls, mixOmics engine) is a dimensionality reduction technique that projects features into a lower-dimensional space, making it sensitive to feature scale and multicollinearity. It is often used for classification tasks with correlated predictors.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data.\nExtract date features (step_date() / step_holiday()): Creates numeric predictors from dates.\nReduce skewness (step_YeoJohnson()): Normalizes skewed predictors for better stability.\nEncode categorical variables (step_dummy()): Converts categorical predictors to numeric format.\nScale features (step_normalize()): Ensures comparable scale, critical for PLS.\nAddress multicollinearity (step_pca()): Optional, as PLS inherently handles multicollinearity, but can further reduce dimensionality."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#maxent-model",
    "href": "posts/05-preprocessing-by-classification-model/main.html#maxent-model",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.8 MaxEnt Model",
    "text": "3.8 MaxEnt Model\nBehavior: The MaxEnt model (maxent, maxnet engine, tidysdm package) is used in species distribution modeling and is robust to some feature transformations but benefits from careful handling of categorical and numeric features.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures complete data.\nExtract date features (step_date() / step_holiday()): Creates predictive features from temporal data.\nReduce skewness (step_YeoJohnson()): Optional, to normalize skewed numeric predictors.\nEncode categorical variables (step_dummy()): Converts categorical predictors to numeric format for maxnet.\nHandle rare categories (step_other() / step_novel()): Manages infrequent or unseen categorical levels.\nScale features (step_normalize()): Optional, but can improve model stability for numeric predictors."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#null-model",
    "href": "posts/05-preprocessing-by-classification-model/main.html#null-model",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3.9 Null Model",
    "text": "3.9 Null Model\nBehavior: The null model (null_model, parsnip engine) is a baseline that predicts the majority class or mean, ignoring predictors. It requires minimal preprocessing.\nRecommended steps (in order):\n\nImpute missing values (step_impute_median() / step_impute_mode()): Ensures compatibility with the workflow, though predictors are not used.\nExtract date features (step_date() / step_holiday()): Optional, only if the target variable depends on temporal features."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#key-takeaways",
    "href": "posts/05-preprocessing-by-classification-model/main.html#key-takeaways",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "5.1 Key Takeaways",
    "text": "5.1 Key Takeaways\n\nTreat preprocessing as part of the model. Integrate recipes into a workflow to ensure consistent transformations across training and prediction.\nGroup transforms by purpose. Organize steps by the problems they address (e.g., missingness, skewness, scale) for clarity and efficiency.\nScale matters for sensitive models. Penalized linear models, SVM, KNN, neural networks, discriminant analysis, and PLS require proper scaling.\nUnderstand engine requirements. Check documentation for specific needs, such as xgboost requiring numeric inputs or ranger handling factors natively."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#practical-checklist",
    "href": "posts/05-preprocessing-by-classification-model/main.html#practical-checklist",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "5.2 Practical Checklist",
    "text": "5.2 Practical Checklist\n\nInspect the dataset: Check for skewness, outliers, missingness, and categorical complexity.\nFor GLMs, GAMs, MARS, SVM, KNN, Neural Networks, LDA/QDA, or PLS: Include step_YeoJohnson() (if skewed) and step_normalize().\nFor xgboost, C5.0, or lightgbm: Ensure categorical variables are encoded (step_dummy()) and missing data is imputed (step_impute_*()).\nFor ranger, rpart, or partykit: Handle missing values, but factors can often remain as-is.\nFor date-heavy datasets: Use step_date() and step_holiday() to extract meaningful features.\nFor complex datasets: Consider step_mutate() for custom transformations or step_discretize() for simplified relationships.\nFor Naive Bayes: Use step_discretize() for non-Gaussian implementations and step_YeoJohnson() for Gaussian assumptions.\nFor MaxEnt: Ensure categorical encoding with step_dummy() and handle missing data.\nFor PLS or discriminant analysis: Include step_normalize() and step_pca() for multicollinearity."
  },
  {
    "objectID": "posts/01-readiable-code/main.html",
    "href": "posts/01-readiable-code/main.html",
    "title": "The Art of Readiable R code",
    "section": "",
    "text": "When we start our journey as programmers it’s normal to get excited by the new possibilities. We get the capacity to do many things that otherwise would be impossible, making projects faster and assuring consistency.\nBut the problems start when you need to modify a script that you wrote 6 months ago. That’s when you find out that you don’t remember why you were applying some specific filters or calculating a value in a odd way.\nAs a Reporting Analyst and I am always creating and changing scripts and after applying the tips provided in The Art of Readable Code by Dustin Boswell and Trevor Foucher I could reduce the time needed to apply changes from 5 to 2 days (60% faster)."
  },
  {
    "objectID": "posts/01-readiable-code/main.html#creating-explicit-names",
    "href": "posts/01-readiable-code/main.html#creating-explicit-names",
    "title": "The Art of Readiable R code",
    "section": "2.1 Creating explicit names",
    "text": "2.1 Creating explicit names\n\n2.1.1 Naming variables\nDefining good variable names is more important than writing a good comment and we should try to give as much context as possible in the variable name. To make this possible:\n\nName based on variable value.\n\nBoolean variables can use words like is, has and should avoid negations. For example: is_integer, has_money and should_end.\nLooping index can have a name followed by the the suffix i. For example: club_i and table_i.\n\nAdd dimensions unit a suffix. For example: price_usd, mass_kg and distance_miles.\nNever change the variable’s value in different sections, instead create a new variable making explicit the change in the name. For example, we can have the variable priceand latter we can create the variable price_discount.\n\nCoding Example\nLet’s apply these points to the mtcars.\n\nmtcars_new_names &lt;-\n  c(\"mpg\" = \"miles_per_gallon\",\n    \"cyl\"= \"cylinders_count\",\n    \"disp\" = \"displacement_in3\",\n    \"hp\" = \"power_hp\",\n    \"drat\" = \"rear_axle_ratio\",\n    \"wt\" = \"weight_klb\",\n    \"qsec\" = \"quarter_mile_secs\",\n    \"vs\" = \"engine_is_straight\",\n    \"am\" = \"transmission_is_manual\",\n    \"gear\" = \"gear_count\",\n    \"carb\" = \"carburetor_count\")\n\nmtcars_renamed &lt;- mtcars\nnames(mtcars_renamed) &lt;- mtcars_new_names[names(mtcars)]\n\nstr(mtcars_renamed)\n\n'data.frame':   32 obs. of  11 variables:\n $ miles_per_gallon      : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cylinders_count       : num  6 6 4 6 8 6 8 4 4 6 ...\n $ displacement_in3      : num  160 160 108 258 360 ...\n $ power_hp              : num  110 110 93 110 175 105 245 62 95 123 ...\n $ rear_axle_ratio       : num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ weight_klb            : num  2.62 2.88 2.32 3.21 3.44 ...\n $ quarter_mile_secs     : num  16.5 17 18.6 19.4 17 ...\n $ engine_is_straight    : num  0 0 1 1 0 1 0 1 1 1 ...\n $ transmission_is_manual: num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear_count            : num  4 4 4 3 3 3 3 4 4 4 ...\n $ carburetor_count      : num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo write good variable names might take some iteration and you might need to play devil’s advocate in under to find out a better name than the initial one.\n\n\n\n\n2.1.2 Defining functions\nCreating explicit functions names can transform a complex process into a simple one.\n\nStart the function with an explicit verb to avoid misunderstandings.\n\n\n\n\nWord\nAlternatives\n\n\n\n\nsend\ndeliver, dispatch, announce, distribute, route\n\n\nfind\nsearch, extract, locate, recover\n\n\nstart\nlaunch, create, begin, open\n\n\nmake\ncreate, set up, build, generate, compose, add, new\n\n\n\n\nThe function name must describe its output.\nA function should do only one thing, otherwise break the functions in more simpler ones to keep the name explicit.\nUse the following words to define range arguments.\n\n\n\n\n\n\n\n\nWord\nUse\n\n\n\n\nmin and max\nUseful to denominate included limits\n\n\nfirst and last\nUseful to denominate exclusive limits\n\n\nbegin and end\nUseful to denominate either inclusive or exclusive limits\n\n\n\nCoding Example\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n  \n  if(!is.data.frame(DF)) stop(\"DF should be a data.frame\")\n  \n  values &lt;- DF[[var_name]]\n  if(!is.numeric(values)) stop(\"var_name should be a numeric column of DF\")\n  \n  min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n  max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n  \n  value_in_range &lt;- \n    values &gt;= min_value & \n    values &lt;= max_value\n  \n  return(DF[value_in_range, ])\n  \n}"
  },
  {
    "objectID": "posts/01-readiable-code/main.html#commenting-correctly",
    "href": "posts/01-readiable-code/main.html#commenting-correctly",
    "title": "The Art of Readiable R code",
    "section": "2.2 Commenting correctly",
    "text": "2.2 Commenting correctly\nThe first step to have a commented project is to have a README file explaining how the code works in a way that to should be enough to present the project to a new team member, but it is also important to add comments to:\n\nExplain how custom functions behave in several situations with minimal examples.\nExplain the reasons behind the decisions that have been taken related to coding style and business logic, like method and constant selection.\nMake explicit pending problems to solve and the initial idea we have to start the solution.\nAvoid commenting bad names, fix them instead.\nSummarize coding sections with a description faster to read than the original code.\n\nCoding Example\nLet’s comment our custom function to explain each point.\n\n# 1. Behavior\n# This function can filter the values of any  data.frame if the var_name\n# is numeric no matter if the column has missing values as it will omit them\n\n# 2. Reasons behind decisions\n# As we are not expecting to make inferences imputation is not necessary.\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n\n  # 5. Reading the code is faster than reading a comment, so we don't need it\n  if(!is.data.frame(DF)) stop(\"DF should be a data.frame\")\n\n  # 2. Reasons behind decisions\n  # We are going to use this vector many times and \n  # saving it as a variable makes the code much easier to read\n  values &lt;- DF[[var_name]]\n  \n  # 5. Reading the code is faster than reading a comment, so we don't need it\n  if(!is.numeric(values)) stop(\"var_name should be a numeric column of DF\")\n  \n  # 2. Reasons behind decisions\n  # Even though a single quantile call could return both values in a vector\n  # it is much simpler to understand if we save each value in a variable\n  min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n  max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n  \n  # 4. The boolean test has an explicit name\n  value_in_range &lt;- \n    values &gt;= min_value & \n    values &lt;= max_value\n  \n  return(DF[value_in_range, ])\n  \n}\n\n\n\n\n\n\n\nNote\n\n\n\nWriting good comments can be challenging, so you better do it in 3 steps:\n\nWrite down whatever comment is on your mind\nRead the comment and see what needs to be improved\nMake the needed improvements"
  },
  {
    "objectID": "posts/01-readiable-code/main.html#code-style",
    "href": "posts/01-readiable-code/main.html#code-style",
    "title": "The Art of Readiable R code",
    "section": "2.3 Code style",
    "text": "2.3 Code style\nIt is important to apply a coding style that make easy to scan the code before going into detail to certain parts. Some advice to improve code style are:\n\nSimilar code should look similar and be grouped in blocks, it will facilitate finding spelling mistakes and prevent repetitive comments.\n\nWe can see how this tips was applied in the keep_rows_in_percentile_range function.\n\nmin_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\nmax_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n\n\nAvoid keeping temporal variables in the global environment .GlobalEnv, instead create a function to make clear the purpose or use pipes (base::|&gt; or magrittr::%&gt;%).\n\nTo ensure this, we created our custom function.\n\nmtcars_renamed |&gt;\n  keep_rows_in_percentile_range(var_name = \"miles_per_gallon\", \n                                min_prob = 0.20,\n                                max_prob = 0.50) |&gt;\n  nrow()\n\n[1] 11\n\n\n\nAvoid writing nested if statements by negating each Boolean test.\n\nIf we hadn’t taken that into consideration our function would be much harder to read.\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n  \n  if(is.data.frame(DF)){\n    \n    values &lt;- DF[[var_name]]\n    \n    if(is.numeric(values)){\n      \n      min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n      max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n      \n      value_in_range &lt;- \n        values &gt;= min_value & \n        values &lt;= max_value\n      \n      return(DF[value_in_range, ])\n      \n    }else{\n      \n      stop(\"var_name should be a numeric column of DF\")\n      \n    }\n\n  }else{\n    \n    stop(\"DF should be a data.frame\")\n    \n  }\n  \n}\n\n\nBoolean validations should be stored in variables should be stored variables to make explicit what was the test about.\nAlways write constants on the right side of the comparison.\nSimplify Boolean comparisons De Morgan’s Law: ! ( A | B) == !A & !B\n\nIn our custom function min_value and max_value works like constants in comparisons to values\n\nvalue_in_range &lt;- \n  values &gt;= min_value & \n  values &lt;= max_value"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models\n\n\n\nMachine Learning\n\nFeature Engineering\n\nData Preprocessing\n\nR Programming\n\nRecipes Package\n\n\n\n\n\n\n\n\n\nAug 6, 2025\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing with {tidymodels} and R\n\n\n\nMachine Learning\n\nFeature Engineering\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nR + Docker: Configuración paso a paso para análisis de datos consistentes\n\n\n\nIntroduction to R\n\nGood Practices\n\n\n\n\n\n\n\n\n\nMar 9, 2025\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Values\n\n\n\nMachine Learning\n\nFeature Engineering\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nThe Art of Readiable R code\n\n\n\nGood Practices\n\nCoding Style\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nAngel Feliz\n\n\n\n\n\nNo matching items"
  }
]