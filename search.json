[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models\n\n\n\nMachine Learning\n\nFeature Engineering\n\nData Preprocessing\n\nR Programming\n\nRecipes Package\n\n\n\n\n\n\n\n\n\nAug 6, 2025\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing with {tidymodels} and R\n\n\n\nMachine Learning\n\nFeature Engineering\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nR + Docker: Configuración paso a paso para análisis de datos consistentes\n\n\n\nIntroduction to R\n\nGood Practices\n\n\n\n\n\n\n\n\n\nMar 9, 2025\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Values\n\n\n\nMachine Learning\n\nFeature Engineering\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nAngel Feliz\n\n\n\n\n\n\n\n\n\n\n\n\nThe Art of Readiable R code\n\n\n\nGood Practices\n\nCoding Style\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nAngel Feliz\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html",
    "href": "posts/03-intro-to-rocker/main.html",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "",
    "text": "El lenguaje R es ampliamente reconocido por su versatilidad en el análisis de datos, y aunque se puede instalar y ejecutar directamente R y RStudio en casi cualquier sistema, usar entornos basados en contenedores —como los que ofrece el Rocker Project— nos brinda ventajas significativas a largo plazo."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#r-una-herramienta-excepcional-para-el-análisis-de-datos",
    "href": "posts/03-intro-to-rocker/main.html#r-una-herramienta-excepcional-para-el-análisis-de-datos",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "1 R: Una herramienta excepcional para el análisis de datos",
    "text": "1 R: Una herramienta excepcional para el análisis de datos\nR destaca por sus numerosas cualidades que lo hacen ideal para científicos de datos y analistas:\n\nGratuito y de código abierto: Permite el acceso libre a un ecosistema de paquetes y herramientas.\nAmplia variedad de herramientas: Existen innumerables paquetes y librerías diseñados para tareas específicas de análisis, estadística y visualización.\nFácil de aprender: Su sintaxis y comunidad de apoyo facilitan el proceso de aprendizaje para nuevos usuarios.\nMultiplataforma: Se ejecuta sin problemas en Windows, macOS y Linux.\nComunidad activa: La colaboración y el constante desarrollo impulsan la innovación y mejora continua del entorno."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#la-importancia-de-la-reproducibilidad-a-lo-largo-del-tiempo",
    "href": "posts/03-intro-to-rocker/main.html#la-importancia-de-la-reproducibilidad-a-lo-largo-del-tiempo",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "2 La importancia de la reproducibilidad a lo largo del tiempo",
    "text": "2 La importancia de la reproducibilidad a lo largo del tiempo\nCuando desarrollamos un programa en R, es fundamental que pueda ser reproducido incluso años después. Para lograrlo, es necesario garantizar:\n\nConsistencia del entorno: Utilizar la misma versión del sistema operativo, de R y de los paquetes instalados.\nPrevención de incompatibilidades: Las actualizaciones y cambios en el software pueden alterar el comportamiento del código.\nVerificación futura: Asegurar que los análisis y resultados se puedan validar y comparar con versiones anteriores.\nIntegridad en la investigación: Una base reproducible es esencial para la honestidad y la claridad en la comunicación científica."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#ventajas-de-usar-entornos-de-desarrollo-en-contenedores",
    "href": "posts/03-intro-to-rocker/main.html#ventajas-de-usar-entornos-de-desarrollo-en-contenedores",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "3 Ventajas de usar entornos de desarrollo en contenedores",
    "text": "3 Ventajas de usar entornos de desarrollo en contenedores\nEl uso de contenedores aporta un gran beneficio para la gestión y evolución de nuestros proyectos:\n\nAislamiento: El entorno de ejecución se mantiene separado del sistema operativo del host, evitando conflictos de dependencias.\nPortabilidad: Un contenedor puede ser ejecutado en cualquier máquina que tenga Docker, sin necesidad de reconfigurar el entorno.\nEscalabilidad: Es sencillo trasladar el entorno local a servidores de mayor capacidad cuando el proyecto crece.\nSeguridad y consistencia: Garantiza que el código se ejecute de manera idéntica, reduciendo errores inesperados y facilitando la colaboración.\nMantenimiento simplificado: Las actualizaciones y cambios se gestionan centralizadamente sin afectar el entorno de producción."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#rocker-simplificando-el-desarrollo-reproducible",
    "href": "posts/03-intro-to-rocker/main.html#rocker-simplificando-el-desarrollo-reproducible",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "4 Rocker: Simplificando el desarrollo reproducible",
    "text": "4 Rocker: Simplificando el desarrollo reproducible\nEl Rocker Project ha hecho el trabajo pesado al proporcionar imágenes Docker preconfiguradas que incluyen R, RStudio y otros entornos especializados.\nPara obtener todos estos beneficios solo tenemos que seguir los siguientes pasos:\n\nInstalar Docker en tu sistema operativo.\n\nSi usas Windows o Mac puedes descargar Docker Desktop desde la página oficial.\nSi usas Linux correr el siguiente bash script con permisos de administrador.\n\n\napt update\nsudo apt install apt-transport-https ca-certificates curl gnupg2 software-properties-common\ncurl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -\nadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/debian\" bookworm stable\napt update\napt-cache policy docker-ce\napt install docker-ce\nsystemctl status docker\n/usr/sbin/usermod -aG docker &lt;YOUR-USER&gt;\nsu - &lt;YOUR-USER&gt;\n\nCrea una carpeta en la que almacenar todos los proyectos de R &lt;mis-proyectos&gt;.\nDentro de &lt;mis-proyectos&gt; crea otra carpeta para almacenar copias de las librerías que irás usando en distintos proyectos &lt;mis-paquetes&gt;.\nDescarga y activa el contenedor a usar mediante el siguiente comando dentro de la línea de comando o dentro de Docker Desktop.\n\ndocker run --rm -tid --name rstudio \\\n  -e DISABLE_AUTH=true \\\n  -p 8787:8787 \\\n  -v &lt;mis-proyectos&gt;:/home/rstudio \\\n  -v &lt;mis-proyectos&gt;/&lt;mis-paquetes&gt;:/opt/r-cache \\\n  ghcr.io/rocker-org/geospatial:4.4.3\n\nAbre una ventana en tu navegador y ve al link http://localhost:8787/ y verás a RStudio funcionando desde el contenedor."
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#configurando-git-y-github",
    "href": "posts/03-intro-to-rocker/main.html#configurando-git-y-github",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "5 Configurando git y github",
    "text": "5 Configurando git y github\nUna vez ya tienes R y RStudio funcionando ya podrás crear tus primeros proyectos y querrás compartir tu progreso con otros.\nAsí como usamos las redes sociales para compartir nuestras experiencias, los programadores tienen una red social llamada GitHub en la que compartimos nuestro código y nos ayudamos mutuamente.\nComo ya el contenedor que estás corriendo tiene instalado git solo necesitas seguir los siguientes pasos para comenzar a utilizar esta poderosa herramienta.\n\nCrea una cuenta en github.com.\nDa clic al ícono con tu cuenta y da clic en Settings.\nAccede a SSH and GPG keys.\nEn la esquina superior derecha dale clic a New SSH key.\nDefine un nombre para tu clave como puede ser &lt;primera-clave&gt;.\nEn RStudio ve a la terminal y ejecuta el comando ssh-keygen y luego da enter para generar una clave pública y una clave privada.\nCopia y pega el contenido de tu clave pública ubicada en la siguiente dirección dentro de la caja de Key:\n\nDesde RStudio: /home/rstudio/.ssh/id_rsa.pub\nDesde tu PC: /.ssh/id_rsa.pub\n\nDa clic en el botón Add SSH key.\nAhora solo queda configurar git con la misma cuenta que tenemos creada en GitHub al correr los siguientes comandos.\n\ngit config --global user.name \"Nombre Apellido\"\ngit config --global user.email \"user@email.com\""
  },
  {
    "objectID": "posts/03-intro-to-rocker/main.html#stopping-container",
    "href": "posts/03-intro-to-rocker/main.html#stopping-container",
    "title": "R + Docker: Configuración paso a paso para análisis de datos consistentes",
    "section": "6 Stopping container",
    "text": "6 Stopping container\nOnce you have completed your task we can stop the container by running.\ndocker stop rstudio"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "",
    "text": "Data preprocessing is the cornerstone of robust machine learning pipelines. Drawing from Max Kuhn’s Applied Predictive Modeling, this guide explores essential transformations through the {tidymodels} lens. We’ll bridge theory with practical implementation, examining when, why, and how to apply each technique while weighing their tradeoffs."
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#centering-and-scaling",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#centering-and-scaling",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "1.1 Centering and Scaling",
    "text": "1.1 Centering and Scaling\nWhen to Use:\n\nModels sensitive to predictor magnitude (SVM, KNN, neural networks)\nBefore dimensionality reduction (PCA) or spatial sign transformations\nWhen predictors have different measurement scales\n\nWhy It Matters:\n\nCenters variables around zero (μ=0)\nStandardizes variance (σ=1)\nEnables meaningful coefficient comparisons\nCritical for distance-based calculations and numerical stability\n\nImplementation:\n\nlibrary(tidymodels)\n\nnorm_recipe &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  prep()\n\nbake(norm_recipe, new_data = NULL) |&gt; summary()\n\n      cyl              disp               hp               drat        \n Min.   :-1.225   Min.   :-1.2879   Min.   :-1.3810   Min.   :-1.5646  \n 1st Qu.:-1.225   1st Qu.:-0.8867   1st Qu.:-0.7320   1st Qu.:-0.9661  \n Median :-0.105   Median :-0.2777   Median :-0.3455   Median : 0.1841  \n Mean   : 0.000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 1.015   3rd Qu.: 0.7688   3rd Qu.: 0.4859   3rd Qu.: 0.6049  \n Max.   : 1.015   Max.   : 1.9468   Max.   : 2.7466   Max.   : 2.4939  \n       wt               qsec                vs               am         \n Min.   :-1.7418   Min.   :-1.87401   Min.   :-0.868   Min.   :-0.8141  \n 1st Qu.:-0.6500   1st Qu.:-0.53513   1st Qu.:-0.868   1st Qu.:-0.8141  \n Median : 0.1101   Median :-0.07765   Median :-0.868   Median :-0.8141  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.000   Mean   : 0.0000  \n 3rd Qu.: 0.4014   3rd Qu.: 0.58830   3rd Qu.: 1.116   3rd Qu.: 1.1899  \n Max.   : 2.2553   Max.   : 2.82675   Max.   : 1.116   Max.   : 1.1899  \n      gear              carb              mpg       \n Min.   :-0.9318   Min.   :-1.1222   Min.   :10.40  \n 1st Qu.:-0.9318   1st Qu.:-0.5030   1st Qu.:15.43  \n Median : 0.4236   Median :-0.5030   Median :19.20  \n Mean   : 0.0000   Mean   : 0.0000   Mean   :20.09  \n 3rd Qu.: 0.4236   3rd Qu.: 0.7352   3rd Qu.:22.80  \n Max.   : 1.7789   Max.   : 3.2117   Max.   :33.90  \n\n\nPros:\n\nRequired for distance-based algorithms\nImproves numerical stability\nFacilitates convergence in gradient-based methods\n\nCons:\n\nLoses original measurement context\nNot needed for tree-based models\nSensitive to outlier influence\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways calculate scaling parameters from training data only to avoid data leakage. Resampling should encapsulate preprocessing steps for honest performance estimation."
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#resolving-skewness",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#resolving-skewness",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "1.2 Resolving Skewness",
    "text": "1.2 Resolving Skewness\nWhen to Use:\n\nSmallest to largest ratio &gt; 20 (max/min)\nRight/left-tailed distributions (|skewness| &gt; 1)\nBefore linear model assumptions\nWhen preparing for PCA or other variance-sensitive methods\n\nSkewness Formula:\n\\[skewness = \\frac{\\sum(x_i - \\bar{x})^3}{(n-1)v^{3/2}} \\quad \\text{where } v = \\frac{\\sum(x_i - \\bar{x})^2}{(n-1)}\\]\nBox-Cox Implementation:\n\ndata(ames, package = \"modeldata\")\n\nskew_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area, data = ames) |&gt;\n  step_BoxCox(Gr_Liv_Area, limits = c(-2, 2)) |&gt; # MLE for λ\n  prep()\n\ntidy(skew_recipe) # Shows selected λ value\n\n# A tibble: 1 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      BoxCox TRUE    FALSE BoxCox_IiiZn\n\n# Calculate original skewness\names |&gt; \n  summarize(skewness = moments::skewness(Gr_Liv_Area))\n\n# A tibble: 1 × 1\n  skewness\n     &lt;dbl&gt;\n1     1.27\n\n\nTransformation Options:\n\nλ=2 → Square\nλ=0.5 → Square root\nλ=-1 → Inverse\nλ=0 → Natural log\n\nPros:\n\nData-driven transformation selection\nHandles zero values gracefully\nContinuous transformation spectrum\n\nCons:\n\nRequires strictly positive values\nLoses interpretability\nSensitive to outlier influence"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#spatial-sign-for-outliers",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#spatial-sign-for-outliers",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "2.1 Spatial Sign for Outliers",
    "text": "2.1 Spatial Sign for Outliers\nWhen to Use:\n\nHigh-dimensional data\nModels sensitive to outlier magnitude (linear regression)\nWhen robust scaling isn’t sufficient\nDealing with radial outliers in multidimensional space\n\nCritical Considerations:\n\nInvestigate outliers for data entry errors first\nConsider cluster validity before removal\nUnderstand missingness mechanism (MCAR/MAR/MNAR)\n\nImplementation:\n\noutlier_recipe &lt;- recipe(Species ~ Sepal.Length + Sepal.Width, data = iris) |&gt;\n  step_normalize(all_numeric()) |&gt; # Mandatory first step\n  step_spatialsign(all_numeric()) |&gt;\n  prep()\n\nbake(outlier_recipe, new_data = NULL) |&gt;\n  ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point()+\n  theme_minimal()\n\n\n\n\n\n\n\n\nPros:\n\nRobust to extreme outliers\nMaintains relative angles\nNon-parametric approach\n\nCons:\n\nDestroys magnitude information\nRequires centered/scaled data\nNot suitable for sparse data"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#pca-for-data-reduction",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#pca-for-data-reduction",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "2.2 PCA for Data Reduction",
    "text": "2.2 PCA for Data Reduction\nOptimal Workflow:\n\nResolve skewness (Box-Cox/Yeo-Johnson)\nCenter/scale predictors\nDetermine components via cross-validation/scree plot\nValidate via resampling\n\nComponent Selection:\n\nRetain components before scree plot elbow\nCumulative variance &gt;80-90%\nCross-validate performance\n\nImplementation:\n\npca_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric(), num_comp = 4L) |&gt; #tune()\n  prep()\n\n# Scree plot visualization\npca_vars &lt;- tidy(pca_recipe, 2, type = \"variance\")\n\npca_vars |&gt; \n  filter(terms == \"percent variance\") |&gt;\n  ggplot(aes(component, value)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Scree Plot\", y = \"% Variance Explained\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Component interpretation\ntidy(pca_recipe, 2) |&gt; \n  filter(component == \"PC1\") |&gt; \n  arrange(-abs(value))\n\n# A tibble: 4 × 4\n  terms         value component id       \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 Petal.Length  0.580 PC1       pca_XioWb\n2 Petal.Width   0.565 PC1       pca_XioWb\n3 Sepal.Length  0.521 PC1       pca_XioWb\n4 Sepal.Width  -0.269 PC1       pca_XioWb\n\n\nPros:\n\nRemoves multicollinearity\nReduces computational load\nReveals latent structure\n\nCons:\n\nLoss of interpretability\nSensitive to scaling\nLinear assumptions\nSupervised methods (PLS) may be preferable for outcome-aware reduction"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#missing-value-imputation",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#missing-value-imputation",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "3.1 Missing Value Imputation",
    "text": "3.1 Missing Value Imputation\nCritical Considerations:\n\nInformative missingness: Is missing pattern related to outcome?\nCensored data: Different treatment than MCAR/MAR\n\n5% missing → Consider removal\n\nType-appropriate methods (KNN vs regression)\n\nImputation Strategies:\n\n\n\nScenario\nApproach\n\n\n\n\n&lt;5% missing\nMedian/mode imputation\n\n\nContinuous predictors\nKNN, linear regression, bagging\n\n\nCategorical\nMode, multinomial logit\n\n\nHigh dimensionality\nRegularized models, MICE\n\n\n\nImplementation:\n\names2 &lt;- ames\names2$Year_Built2 &lt;- ames2$Year_Built\n\nset.seed(5858)\names2[sample.int(2930, 1000), c(\"Year_Built2\")] &lt;- NA_integer_\names2[sample.int(2930, 800), c(\"Lot_Frontage\")] &lt;- NA_integer_\n\nimpute_recipe &lt;- recipe(Sale_Price ~ Lot_Frontage + Year_Built2 + Year_Built, data = ames2) |&gt;\n  step_impute_knn(Lot_Frontage, neighbors = 3L) |&gt; #tune()\n  step_impute_linear(Year_Built2, impute_with = imp_vars(Year_Built)) |&gt;\n  prep()\n\n# Assess imputation quality\ncomplete_data &lt;- bake(impute_recipe, new_data = ames2)\ncor(complete_data$Year_Built, complete_data$Year_Built2, use = \"complete.obs\")\n\n[1] 1\n\ncor(complete_data$Lot_Frontage, ames$Lot_Frontage, use = \"complete.obs\")\n\n[1] 0.8296254"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#feature-filtering",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#feature-filtering",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "3.2 Feature Filtering",
    "text": "3.2 Feature Filtering\nNear-Zero Variance Detection:\n\nFrequency ratio &gt; 20\nUnique values &lt; 10%\nPercent unique = n_unique/n * 100\n\n\nnzv_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_nzv(all_predictors(), freq_cut = 95/5, unique_cut = 10) |&gt;\n  prep()\n\ntidy(nzv_recipe)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id       \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;    \n1      1 step      nzv   TRUE    FALSE nzv_sEXIo\n\n\nMulticollinearity Handling:\n\nVariance Inflation Factor (VIF) &gt; 5-10\nPairwise correlation threshold\nIterative removal algorithm\n\n\ncorr_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_corr(all_numeric(), threshold = 0.9, method = \"spearman\") |&gt;\n  prep()\n\ntidy(corr_recipe)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id        \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;     \n1      1 step      corr  TRUE    FALSE corr_aKa2d"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#categorical-encoding-nonlinear-terms",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#categorical-encoding-nonlinear-terms",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "4.1 Categorical Encoding & Nonlinear Terms",
    "text": "4.1 Categorical Encoding & Nonlinear Terms\nBest Practices:\n\nDummy variables for nominal predictors (one-hot encoding)\nOrdered factors for ordinal categories\nInclude interaction terms where domain knowledge suggests\nAdd polynomial terms for known nonlinear relationships\n\nExample:\n\nnonlinear_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_dummy(all_nominal(), -all_outcomes()) |&gt;\n  step_poly(Sepal.Length, degree = 2) |&gt;\n  step_interact(~ Sepal.Width:Petal.Length) |&gt;\n  prep()"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#distance-to-class-centroids",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#distance-to-class-centroids",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "4.2 Distance to Class Centroids",
    "text": "4.2 Distance to Class Centroids\nWhen to Use:\n\nClassification problems\nCluster-aware feature engineering\nImproving linear separability\nAugmenting existing feature set\n\nImplementation:\n\ncentroid_recipe &lt;- recipe(Species ~ ., data = iris) |&gt;\n  step_classdist(all_numeric(), class = \"Species\", pool = FALSE) |&gt;\n  prep()\n\nbake(centroid_recipe, new_data = NULL) |&gt;\n  select(starts_with(\"classdist_\")) |&gt;\n  head()\n\n# A tibble: 6 × 3\n  classdist_setosa classdist_versicolor classdist_virginica\n             &lt;dbl&gt;                &lt;dbl&gt;               &lt;dbl&gt;\n1           -0.800                 4.74                5.21\n2            0.733                 4.42                5.04\n3            0.250                 4.55                5.08\n4            0.534                 4.42                4.95\n5           -0.272                 4.79                5.22\n6            1.31                  4.79                5.21"
  },
  {
    "objectID": "posts/04-data-preprocessing-with-tidymodels/main.html#binning-strategies",
    "href": "posts/04-data-preprocessing-with-tidymodels/main.html#binning-strategies",
    "title": "Data Preprocessing with {tidymodels} and R",
    "section": "4.3 Binning Strategies",
    "text": "4.3 Binning Strategies\nWhen to Avoid:\n\nManual binning pre-analysis\nWith tree-based models\nSmall sample sizes\nWhen interpretability trumps accuracy\n\nEthical Considerations:\n\nMedical diagnostics require maximum accuracy\nLegal implications of arbitrary thresholds\nPotential bias introduction through careless discretization\n\nSmart Discretization:\n\nbin_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area, data = ames) |&gt;\n  step_discretize(Gr_Liv_Area, num_breaks = 4, min_unique = 10) |&gt;\n  prep()\n\nbake(bin_recipe, new_data = NULL) |&gt;\n  count(Gr_Liv_Area)\n\n# A tibble: 4 × 2\n  Gr_Liv_Area     n\n  &lt;fct&gt;       &lt;int&gt;\n1 bin1          735\n2 bin2          733\n3 bin3          729\n4 bin4          733"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angel Feliz",
    "section": "",
    "text": "I’m a Data Scientist specializing in R programming with experience in reporting automation, data visualization, and package development. Currently working remotely as a Leasing Reporting Manager at Phoenix Tower International, where I lead development of R packages to improve critical business reports and data workflows.\n\n\nStarting as a junior data analyst, I’ve grown into a data professional who excels at automating processes, ensuring data quality, and delivering actionable insights. My experience spans financial reporting, data migration, and building validation frameworks that have significantly reduced manual effort while improving accuracy.\nI’m passionate about open source contribution and have authored several R packages, including:\n\nbiblegatewayr: For scraping Bible verses\ntidyvalidate: For identifying business errors in tables\ncorrcat: For exploring correlations between categorical variables\n\nI’ve also contributed to the {data.table} package, writing the primary documentation for join operations.\n\n\n\nI’m currently working on a data science portfolio project focused on increasing NYC taxi drivers’ earnings through data-driven strategies. Using the CRISP-DM methodology, I’m analyzing rideshare data from Juno, Uber, Via and Lyft to determine:\n\nHow drivers can increase monthly earnings by selectively skipping trips?\nHow changing initial locations and timing can optimize earnings?\n\nThis project showcases my skills with the R ecosystem, particularly with:\n\nTidyverse packages for data manipulation and visualization\nGeospatial analysis using sf, leaflet, and tmap\nStatistical modeling and inference\nDatabase connectivity and high-performance data manipulation\n\n\n\n\nI hold a Bachelor’s in Industrial Engineering from Instituto Tecnológico de Santo Domingo and am fluent in both English and Spanish. I’m certified in data science, R programming, and data analysis through DataCamp."
  },
  {
    "objectID": "index.html#professional-journey",
    "href": "index.html#professional-journey",
    "title": "Angel Feliz",
    "section": "",
    "text": "Starting as a junior data analyst, I’ve grown into a data professional who excels at automating processes, ensuring data quality, and delivering actionable insights. My experience spans financial reporting, data migration, and building validation frameworks that have significantly reduced manual effort while improving accuracy.\nI’m passionate about open source contribution and have authored several R packages, including:\n\nbiblegatewayr: For scraping Bible verses\ntidyvalidate: For identifying business errors in tables\ncorrcat: For exploring correlations between categorical variables\n\nI’ve also contributed to the {data.table} package, writing the primary documentation for join operations."
  },
  {
    "objectID": "index.html#current-project",
    "href": "index.html#current-project",
    "title": "Angel Feliz",
    "section": "",
    "text": "I’m currently working on a data science portfolio project focused on increasing NYC taxi drivers’ earnings through data-driven strategies. Using the CRISP-DM methodology, I’m analyzing rideshare data from Juno, Uber, Via and Lyft to determine:\n\nHow drivers can increase monthly earnings by selectively skipping trips?\nHow changing initial locations and timing can optimize earnings?\n\nThis project showcases my skills with the R ecosystem, particularly with:\n\nTidyverse packages for data manipulation and visualization\nGeospatial analysis using sf, leaflet, and tmap\nStatistical modeling and inference\nDatabase connectivity and high-performance data manipulation"
  },
  {
    "objectID": "index.html#skills-education",
    "href": "index.html#skills-education",
    "title": "Angel Feliz",
    "section": "",
    "text": "I hold a Bachelor’s in Industrial Engineering from Instituto Tecnológico de Santo Domingo and am fluent in both English and Spanish. I’m certified in data science, R programming, and data analysis through DataCamp."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html",
    "href": "posts/05-preprocessing-by-classification-model/main.html",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "",
    "text": "If you’re diving into classification tasks with R, you know that preprocessing your data is critical to building effective machine learning models. The recipes package is my go-to tool for crafting preprocessing pipelines that prepare data for classification, ensuring models like Random Forests, SVMs, or Naive Bayes can accurately distinguish between classes. In this guide, I’ll walk you through how to use recipes to preprocess data for a range of classification models, focusing on binary and multi-class problems. Whether you’re a beginner or a seasoned data scientist, this post is a reference for you (and me!) to revisit when tackling classification challenges.\nWhy focus on classification? Classification tasks—like predicting whether an email is spam or categorizing customer behavior—are common in data science, and each model has unique data requirements. Preprocessing ensures your data meets these needs, handles class imbalance, and minimizes noise. I’ve included practical tips, clarified step ordering, and balanced computational cost with performance to make this guide actionable. Let’s dive into crafting the perfect preprocessing pipeline for classification!"
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#why-preprocessing-matters-for-classification",
    "href": "posts/05-preprocessing-by-classification-model/main.html#why-preprocessing-matters-for-classification",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "1 Why Preprocessing Matters for Classification",
    "text": "1 Why Preprocessing Matters for Classification\nClassification models aim to predict discrete class labels (e.g., “positive” vs. “negative” or multiple categories like “low,” “medium,” “high”). Each model has assumptions about the data: logistic regression needs scaled, numeric inputs; tree-based models handle categoricals well but struggle with rare categories; SVMs and neural networks are sensitive to feature scales. A well-designed preprocessing pipeline ensures your data is clean, appropriately formatted, and optimized for class separation, while addressing issues like missing values or imbalanced classes.\nThe recipes package in R lets you define a reproducible sequence of preprocessing steps, like a recipe for your favorite dish. Each step is tailored to your model’s needs, and the order matters—imputation before transformation, transformation before normalization. This guide focuses on classification-specific preprocessing, with tips for handling binary and multi-class problems and evaluating performance with metrics like AUC or F1-score during cross-validation.\nPro Tip: Always cross-validate your preprocessing pipeline with your model to ensure robust performance. Use classification metrics (e.g., AUC, precision, recall) to evaluate how well your pipeline supports class separation."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#understanding-model-functions-and-their-engines",
    "href": "posts/05-preprocessing-by-classification-model/main.html#understanding-model-functions-and-their-engines",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "2 Understanding Model Functions and Their Engines",
    "text": "2 Understanding Model Functions and Their Engines\nBefore defining a preprocessing strategy, it’s crucial to understand how different classification models function and the engines that power them. This knowledge helps tailor preprocessing steps to each model’s requirements. Below is a table summarizing key classification models and their associated engines, which implement the underlying algorithms.\n\n\n\nModel\nEngine(s)\n\n\n\n\nC5_rules\nC5.0\n\n\nbag_mars\nearth\n\n\nbag_mlp\nnnet\n\n\nbag_tree\nC5.0, rpart\n\n\nbart\ndbarts\n\n\nboost_tree\nC5.0, lightgbm, xgboost\n\n\ndecision_tree\nC5.0, partykit, rpart\n\n\ndiscrim_flexible\nearth\n\n\ndiscrim_linear\nMASS, mda, sda, sparsediscrim\n\n\ndiscrim_quad\nMASS, sparsediscrim\n\n\ndiscrim_regularized\nklaR\n\n\ngen_additive_mod\nmgcv\n\n\nlogistic_reg\nbrulee, glm, glmnet, LiblineaR, stan\n\n\nmars\nearth\n\n\nmaxent\nmaxnet\n\n\nmlp\nbrulee, brulee_two_layer, nnet\n\n\nmultinom_reg\nbrulee, glmnet, nnet\n\n\nnaive_Bayes\nklaR, naivebayes\n\n\nnearest_neighbor\nkknn\n\n\npls\nmixOmics\n\n\nrand_forest\naorsf, partykit, randomForest, ranger\n\n\nrule_fit\nxrf\n\n\nsvm_linear\nkernlab, LiblineaR\n\n\nsvm_poly\nkernlab\n\n\nsvm_rbf\nkernlab, liquidSVM\n\n\n\nKey Insights:\n\nTree-Based Models (e.g., C5_rules, boost_tree, rand_forest): Engines like C5.0, xgboost, and ranger handle categorical variables natively but benefit from cardinality reduction and imputation.\nLinear Models (e.g., logistic_reg, discrim_linear): Engines like glmnet and MASS require numeric, scaled inputs and are sensitive to multicollinearity.\nNeural Networks (e.g., mlp, bag_mlp): Engines like nnet and brulee demand strict normalization and complete data.\nKernel-Based Models (e.g., svm_rbf, svm_poly): Engines like kernlab rely on distance metrics, making scaling and transformation critical.\nSpecialized Models (e.g., maxent, gen_additive_mod): Engines like maxnet and mgcv have unique requirements, such as smooth terms or specific feature formats.\n\nUnderstanding these engines guides preprocessing choices, ensuring compatibility and optimal performance. For example, C5.0 in C5_rules handles missing values internally but benefits from explicit imputation, while glmnet in logistic_reg requires scaled inputs for regularization."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#tree-based-rule-based-classification-models",
    "href": "posts/05-preprocessing-by-classification-model/main.html#tree-based-rule-based-classification-models",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "3 Tree-Based & Rule-Based Classification Models",
    "text": "3 Tree-Based & Rule-Based Classification Models\nTree-based models (e.g., Random Forests, XGBoost) and rule-based models (e.g., C5.0) split data into regions based on feature values. They’re robust to unscaled data but sensitive to high-cardinality categories and uninformative features. Here’s how to preprocess for classification.\n\n3.1 C5.0 Rules\nGoal: Generate interpretable IF-THEN rules for classification (e.g., “IF age &lt; 30 AND is_holiday = TRUE THEN positive”). Powered by the C5.0 engine.\n\nImputation (step_impute_mode or step_impute_bag): C5.0 can’t handle missing values. Use step_impute_mode for categorical variables (simple and fast) or step_impute_bag for tree-based imputation that preserves relationships for classification.\nDate Features (step_date, step_holiday): Extract features like day of week or holiday flags for rule-based splits (e.g., “IF month = December THEN…”).\nDiscretization (step_discretize): Bin numeric variables into categories (e.g., age into “Young,” “Middle-Aged,” “Senior”) to simplify rules and improve generalization. Tune bin thresholds to balance granularity and overfitting.\nHigh Cardinality (step_other): Collapse rare categorical levels into “other” to prevent overly specific rules.\nNovel Categories (step_novel): Add a level for unseen categories in production to avoid errors.\nZero Variance (step_zv): Remove predictors with one value—they’re useless for rules.\nDummy Encoding (step_dummy, optional): C5.0 handles categoricals natively, so skip unless sharing the recipe with other models.\nCorrelation (step_corr, optional): Remove correlated predictors for simpler rules, but use sparingly as C5.0 is robust to multicollinearity.\n\nStep Order: Imputation → Date Features → Discretization → High Cardinality → Novel Categories → Zero Variance → Dummy Encoding → Correlation.\nWhy It Works: These steps create clean, categorical data for interpretable rules, with step_discretize and step_other preventing overfitting.\nTip: Prioritize interpretability by avoiding step_dummy. Tune step_discretize bins with cross-validation to optimize class separation.\n\n\n3.2 Bagged MARS (Classification)\nGoal: Average multiple Multivariate Adaptive Regression Splines (MARS) models for robust class predictions, using the earth engine.\nMARS fits piecewise linear functions for non-linear class boundaries, with bagging reducing variance. For classification, MARS uses a logistic link.\n\nImputation (step_impute_knn): Use step_impute_knn to impute based on similar data points, aligning with MARS’s local relationships. Avoid step_impute_linear unless data has clear linear trends.\nDate and Spatial Features (step_date, step_holiday, step_geodist): Create numeric features (e.g., day of year, spatial distances) for MARS splits.\nSplines (step_bs or step_ns): Pre-engineer splines to guide MARS’s non-linear splits. Tune degrees of freedom.\nInteractions (step_interact, optional): Add specific interactions (e.g., temperature * humidity) if domain knowledge suggests they improve class separation.\nNormalization (step_normalize, optional): Stabilizes knot placement but can be skipped for computational efficiency.\nNear-Zero Variance (step_nzv): Remove low-variance predictors to reduce noise.\nCorrelation (step_corr): Remove correlated predictors to stabilize MARS’s feature selection.\nSampling (step_sample): Address class imbalance with SMOTE or ROSE to ensure balanced class predictions.\n\nStep Order: Imputation → Date/Spatial Features → Splines → Interactions → Sampling → Normalization → Near-Zero Variance → Correlation.\nWhy It Works: These steps provide clean, numeric data for MARS’s non-linear splits, with step_sample ensuring robust handling of imbalanced classes.\nTip: Use step_geodist only with spatial data. Tune spline parameters and sampling to optimize AUC or F1-score.\n\n\n3.3 Bagged MLP\nGoal: Stabilize Multi-Layer Perceptrons (neural networks) for classification by averaging bootstrap samples, using the nnet engine.\nPreprocessing is identical to a single MLP (see below), as each network needs scaled, numeric data. Bagging reduces variance from random weights.\nTip: Bagging increases computational cost—test if the robustness justifies the time for your dataset.\n\n\n3.4 Bagged Decision Trees\nGoal: Build an ensemble of decision trees, foundational for Random Forests, using C5.0 or rpart engines.\n\nImputation (step_impute_median or step_impute_roll): Use step_impute_median for robustness in non-temporal data or step_impute_roll for time-series.\nDate Features (step_date, step_holiday): Create features for tree splits.\nHigh Cardinality (step_other): Collapse rare categories to improve generalization.\nNovel Categories (step_novel or step_unknown): Handle new factor levels.\nZero Variance (step_zv): Remove uninformative predictors.\nDummy Encoding (step_dummy): Optional, as some engines (e.g., catboost) handle categoricals natively.\nSampling (step_sample): Use SMOTE or ROSE for class imbalance. Tune sampling parameters.\nDiscretization (step_cut or step_discretize, optional): Pre-bin numerics to reduce noise if domain knowledge supports thresholds.\n\nStep Order: Imputation → Date Features → High Cardinality → Novel Categories → Zero Variance → Dummy Encoding → Sampling → Discretization.\nWhy It Works: These steps ensure clean data for tree splits, with step_sample critical for imbalanced classes.\nTip: Ensure step_impute_roll aligns with temporal order. Evaluate sampling with classification metrics like F1-score.\n\n\n3.5 Boosted Trees (e.g., XGBoost, LightGBM)\nGoal: Sequentially build trees to correct classification errors, using C5.0, lightgbm, or xgboost engines.\n\nImputation (step_impute_median): Explicit imputation stabilizes results, though boosting handles missing values internally.\nDate Features (step_date, step_holiday): Create temporal features for splits.\nDummy Encoding (step_dummy or step_dummy_multi_choice): Convert categoricals, with step_dummy_multi_choice for multi-label variables.\nZero Variance (step_zv): Remove uninformative predictors.\nHigh Cardinality (step_other): Prevent overfitting to rare categories.\nInteractions (step_interact, optional): Add known interactions to guide boosting.\nSampling (step_sample): Address class imbalance for better class predictions.\n\nStep Order: Imputation → Date Features → High Cardinality → Dummy Encoding → Zero Variance → Interactions → Sampling.\nWhy It Works: These steps provide clean data and manage cardinality, with step_sample ensuring balanced class learning.\nTip: Test if internal missing value handling outperforms imputation. Use step_dummy_multi_choice only for multi-label variables.\n\n\n3.6 Single Decision Tree\nGoal: Build a single tree for classification, prone to overfitting, using C5.0, partykit, or rpart engines.\n\nImputation (step_impute_mode or step_impute_median): Use step_impute_mode for categoricals, step_impute_median for numerics.\nDate Features (step_date, step_holiday): Create split-friendly features.\nHigh Cardinality (step_other): Essential to prevent overfitting.\nNovel Categories (step_unknown): Handle new levels.\nZero/Near-Zero Variance (step_zv, step_nzv): Remove uninformative predictors.\nNumeric to Factor (step_num2factor, optional): Treat discrete numerics as categorical (e.g., number of cylinders).\nSpatial Features (step_depth, optional): Convert coordinates to a single feature.\n\nStep Order: Imputation → Date Features → High Cardinality → Novel Categories → Zero/Near-Zero Variance → Numeric to Factor → Spatial Features.\nWhy It Works: These steps reduce overfitting by cleaning data and controlling cardinality.\nTip: Single trees are rarely used—consider Random Forests for better performance.\n\n\n3.7 Random Forest\nGoal: Build de-correlated trees for robust classification, using aorsf, partykit, randomForest, or ranger engines.\n\nCharacter to Factor (step_string2factor): Convert character data to factors.\nImputation (step_impute_median or step_impute_bag): Use step_impute_median for speed or step_impute_bag for complex missingness.\nDate Features (step_date, step_holiday): Create temporal features.\nHigh Cardinality (step_other): Prevent overfitting to rare categories.\nNovel Categories (step_novel): Handle new levels.\nZero Variance (step_zv): Remove uninformative predictors.\nSampling (step_sample): Address class imbalance with SMOTE or ROSE.\n\nStep Order: Character to Factor → Imputation → Date Features → High Cardinality → Novel Categories → Zero Variance → Sampling.\nWhy It Works: These steps ensure clean, generalizable data, with step_sample critical for imbalanced classes.\nTip: Test step_impute_median vs. step_impute_bag for computational efficiency. Use step_select_roc for high-dimensional data.\n\n\n3.8 RuleFit\nGoal: Combine tree-based rules with LASSO for classification, using the xrf engine.\n\nImputation (step_impute_median): Ensure complete data for LASSO.\nDate Features (step_date, step_holiday): Create base features for rules.\nDummy Encoding (step_dummy): Convert categoricals for LASSO.\nNormalization (step_normalize): Essential for fair LASSO penalization.\nZero Variance (step_zv): Remove uninformative predictors.\nCorrelation (step_corr): Reduce multicollinearity for LASSO stability.\nInteractions (step_interact, optional): Seed important interactions for LASSO.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Interactions → Dummy Encoding → Normalization → Zero Variance → Correlation → Sampling.\nWhy It Works: These steps balance tree rules and LASSO, with step_normalize ensuring fair penalization.\nTip: Tune the number of rules in RuleFit. Use step_sample for imbalanced classes."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#linear-generalized-linear-classification-models",
    "href": "posts/05-preprocessing-by-classification-model/main.html#linear-generalized-linear-classification-models",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "4 Linear & Generalized Linear Classification Models",
    "text": "4 Linear & Generalized Linear Classification Models\nLinear models assume a linear relationship between predictors and class probabilities (via a link function). They’re sensitive to scale and multicollinearity.\n\n4.1 Bayesian Additive Regression Trees (BART)\nGoal: Use Bayesian priors for robust classification with tree ensembles, using the dbarts engine.\n\nImputation (step_impute_bag): Tree-based imputation aligns with BART’s methodology.\nDate Features (step_date, step_holiday): Create features for temporal splits.\nDummy Encoding (step_dummy): Convert categoricals to numeric.\nNormalization (step_normalize, optional): Aids Bayesian priors and MCMC sampling.\nCorrelation (step_corr, optional): Speeds up MCMC by reducing multicollinearity.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Dummy Encoding → Sampling → Normalization → Correlation.\nWhy It Works: These steps provide clean, numeric data, with step_impute_bag and step_sample enhancing classification performance.\nTip: Test if step_normalize improves MCMC efficiency. Use step_sample for multi-class problems.\n\n\n4.2 Logistic Regression\nGoal: Model log-odds for binary classification, using brulee, glm, glmnet, LiblineaR, or stan engines.\n\nImputation (step_impute_median): Ensure complete data with robust imputation.\nDate Features (step_date, step_holiday): Create numeric predictors.\nReference Level (step_relevel): Set reference categories for interpretability.\nDummy Encoding (step_dummy): Convert categoricals to 0/1.\nZero Variance (step_zv): Remove uninformative predictors.\nCorrelation (step_corr): Stabilize coefficients.\nNormalization (step_normalize or step_center & step_scale): Essential for regularization and convergence.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Reference Level → Dummy Encoding → Sampling → Normalization → Zero Variance → Correlation.\nWhy It Works: These steps ensure scaled, numeric data for stable logistic regression, with step_sample handling imbalance.\nTip: Use step_relevel for interpretable coefficients. Evaluate with AUC or F1-score.\n\n\n4.3 Multinomial Logistic Regression\nGoal: Model multi-class probabilities, using brulee, glmnet, or nnet engines.\n\nImputation (step_impute_median): Ensure complete data.\nDate Features (step_date, step_holiday): Create covariates.\nReference Level (step_relevel): Set reference categories.\nDummy Encoding (step_dummy, step_dummy_extract): Handle categoricals and multi-label variables.\nZero Variance (step_zv): Remove uninformative predictors.\nCorrelation (step_corr): Stabilize coefficients.\nNormalization (step_normalize): Essential for regularization.\nPLS (step_pls, optional): Reduce dimensionality for multi-class problems.\nOrdinal Scores (step_ordinalscore, optional): Convert ordered categoricals to numeric.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Reference Level → Dummy Encoding → Ordinal Scores → Sampling → Normalization → Zero Variance → Correlation → PLS.\nWhy It Works: These steps support multi-class classification, with step_pls and step_sample enhancing performance.\nTip: Use step_dummy_extract for multi-label variables. Tune step_pls components.\n\n\n4.4 Partial Least Squares (PLS)\nGoal: Handle multicollinearity in classification, using the mixOmics engine.\n\nImputation (step_impute_median): Ensure complete data.\nScaling (step_scale, step_center): Essential for covariance calculations.\nDummy Encoding (step_dummy): Convert categoricals.\nDate Features (step_date, step_holiday): Create features for classification.\nNear-Zero Variance (step_nzv): Remove low-variance predictors.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Dummy Encoding → Sampling → Scaling → Near-Zero Variance.\nWhy It Works: These steps ensure scaled, numeric data for PLS’s covariance-based components, with step_sample for balanced classes.\nTip: Tune the number of PLS components. Use step_sample for multi-class problems."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#discriminant-analysis-classification-models",
    "href": "posts/05-preprocessing-by-classification-model/main.html#discriminant-analysis-classification-models",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "5 Discriminant Analysis Classification Models",
    "text": "5 Discriminant Analysis Classification Models\nThese models model predictor distributions for each class, often assuming normality.\n\n5.1 Flexible Discriminant Analysis (FDA)\nGoal: Use splines for non-linear class boundaries, using the earth engine.\n\nImputation (step_impute_knn): Preserves local structure.\nSplines (step_spline_monotone or step_ns): Create non-linear boundaries. Tune degrees of freedom.\nNormalization (step_normalize or step_scale): Ensure comparable basis functions.\nDummy Encoding (step_dummy): Convert categoricals.\nNear-Zero Variance (step_nzv): Remove low-variance predictors.\nCorrelation (step_corr): Reduce multicollinearity.\nTransformation (step_BoxCox or step_YeoJohnson): Improve normality for LDA component.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Transformation → Dummy Encoding → Correlation → Splines → Sampling → Normalization → Near-Zero Variance.\nWhy It Works: These steps enable non-linear boundaries while supporting LDA’s normality assumption.\nTip: Use step_YeoJohnson for flexibility with negative values. Tune spline parameters.\n\n\n5.2 Linear Discriminant Analysis (LDA)\nGoal: Assume normal predictors with common covariance, using MASS, mda, sda, or sparsediscrim engines.\n\nImputation (step_impute_knn): Preserves local structure.\nTransformation (step_BoxCox or step_YeoJohnson): Improve normality.\nScaling (step_center, step_scale): Prevent large-variance predictors from dominating.\nCorrelation (step_corr or step_lincomb): Avoid singular matrices.\nSpatial Sign (step_spatialsign): Robust to outliers.\nDummy Encoding (step_dummy): Convert categoricals.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Transformation → Dummy Encoding → Correlation → Sampling → Scaling → Spatial Sign.\nWhy It Works: These steps align with LDA’s normality and covariance assumptions.\nTip: Check the equal covariance assumption. Use step_spatialsign cautiously due to interpretability.\n\n\n5.3 Quadratic Discriminant Analysis (QDA)\nGoal: Allow class-specific covariances, using MASS or sparsediscrim engines.\n\nImputation (step_impute_knn): Preserves local structure.\nPCA (step_pca): Stabilize covariance estimation. Tune components.\nTransformation (step_BoxCox or step_YeoJohnson): Improve normality.\nZero Variance (step_zv): Remove uninformative predictors.\nCorrelation (step_corr): Prevent singular matrices.\nScaling (step_scale): Ensure fair covariance calculations.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Transformation → Dummy Encoding → Correlation → PCA → Sampling → Scaling → Zero Variance.\nWhy It Works: These steps stabilize QDA’s covariance estimation, with step_pca critical for high-dimensional data.\nTip: Use step_pca for small sample sizes. step_YeoJohnson handles negative values.\n\n\n5.4 Regularized Discriminant Analysis (RDA)\nGoal: Hybrid of LDA and QDA with shrinkage, using the klaR engine.\n\nImputation (step_impute_knn): Preserves local structure.\nScaling (step_center, step_scale): Essential for regularization.\nTransformation (step_BoxCox or step_YeoJohnson): Improve normality.\nCorrelation (step_corr, step_lincomb): Prevent singular matrices.\nNear-Zero Variance (step_nzv): Remove low-variance predictors.\nDummy Encoding (step_dummy): Convert categoricals.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Transformation → Dummy Encoding → Correlation → Sampling → Scaling → Near-Zero Variance.\nWhy It Works: These steps balance LDA and QDA assumptions, with scaling ensuring proper regularization.\nTip: Tune RDA’s regularization parameters. Use step_sample for multi-class problems."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#neural-networks-kernel-based-classification-models",
    "href": "posts/05-preprocessing-by-classification-model/main.html#neural-networks-kernel-based-classification-models",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "6 Neural Networks & Kernel-Based Classification Models",
    "text": "6 Neural Networks & Kernel-Based Classification Models\nThese models are powerful but sensitive to feature scaling.\n\n6.1 Multi-Layer Perceptron (MLP)\nGoal: Universal approximator for non-linear class boundaries, using brulee, brulee_two_layer, or nnet engines.\n\nImputation (step_impute_knn or step_impute_bag): Ensure complete data.\nDummy Encoding (step_dummy): Convert categoricals.\nDate Features (step_date, step_holiday, step_harmonic): Create numeric features, with step_harmonic for cyclical patterns.\nTransformation (step_YeoJohnson): Reduce skewness before normalization.\nNormalization (step_normalize or step_scale & step_center): Non-negotiable for gradient descent.\nDimensionality Reduction (step_pca or step_ica): Reduce input nodes. Tune components.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Transformation → Dummy Encoding → Sampling → Dimensionality Reduction → Normalization.\nWhy It Works: These steps ensure scaled, numeric data for stable training, with step_sample for balanced classes.\nTip: Use step_harmonic for cyclical features. Test step_ica for independence assumptions.\n\n\n6.2 Linear SVM\nGoal: Find a hyperplane for class separation, using kernlab or LiblineaR engines.\n\nImputation (step_impute_knn): Aligns with SVM’s distance-based nature.\nDummy Encoding (step_dummy): Convert categoricals.\nScaling (step_scale, step_center): Non-negotiable for distance calculations.\nZero Variance (step_zv): Remove uninformative predictors.\nCorrelation (step_corr): Simplify optimization.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Dummy Encoding → Correlation → Sampling → Scaling → Zero Variance.\nWhy It Works: Scaling ensures fair distance calculations, with step_sample for balanced classes.\nTip: Evaluate with AUC to optimize the margin.\n\n\n6.3 Polynomial SVM\nGoal: Use a polynomial kernel for non-linear class separation, using the kernlab engine.\n\nImputation (step_impute_knn): More robust than step_impute_linear.\nDummy Encoding (step_dummy): Convert categoricals.\nTransformation (step_YeoJohnson): Reduce skewness before normalization.\nNormalization (step_normalize): Non-negotiable for dot products.\nNear-Zero Variance (step_nzv): Remove low-variance predictors.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Transformation → Dummy Encoding → Sampling → Normalization → Near-Zero Variance.\nWhy It Works: Scaling and transformation ensure a clean feature space for the polynomial kernel.\nTip: step_impute_knn aligns with the kernel’s non-linear nature. Tune kernel parameters.\n\n\n6.4 RBF SVM\nGoal: Use an RBF kernel for non-linear class separation, using kernlab or liquidSVM engines.\n\nImputation (step_impute_knn): Aligns with Euclidean distance.\nDummy Encoding (step_dummy): Convert categoricals.\nTransformation (step_YeoJohnson): Reduce skew and outliers.\nNormalization (step_normalize): Non-negotiable for distance calculations.\nSpatial Features (step_geodist, optional): For spatial data.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Transformation → Dummy Encoding → Spatial Features → Sampling → Normalization.\nWhy It Works: These steps ensure a scaled feature space for the RBF kernel, with step_sample for balanced classes.\nTip: Use step_YeoJohnson for flexibility with negative values. Tune the gamma parameter."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#other-classification-model-types",
    "href": "posts/05-preprocessing-by-classification-model/main.html#other-classification-model-types",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "7 Other Classification Model Types",
    "text": "7 Other Classification Model Types\n\n7.1 Generalized Additive Models (GAMs)\nGoal: Model class probabilities with smooth functions, using the mgcv engine.\n\nImputation (step_impute_knn): Preserves non-linear relationships.\nSplines (step_ns): Create smooth terms for class boundaries. Tune degrees of freedom.\nInteractions (step_te): Model smooth interaction surfaces.\nDate Features (step_date, step_holiday): Create base features.\nDummy Encoding (step_dummy): For parametric terms.\nNormalization (step_normalize, optional): For parametric terms.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Splines → Interactions → Dummy Encoding → Sampling → Normalization.\nWhy It Works: Splines and interactions model non-linear class boundaries, with step_sample for balanced classes.\nTip: Tune spline degrees of freedom. Use step_sample for multi-class problems.\n\n\n7.2 MARS (Classification)\nGoal: Fit piecewise linear functions for class boundaries, using the earth engine.\nSee Bagged MARS. Steps are identical, but a single MARS model is faster and more prone to overfitting.\nTip: Use step_nzv and step_corr to reduce overfitting. Apply step_sample for imbalanced classes.\n\n\n7.3 Maximum Entropy (MaxEnt)\nGoal: Model class probabilities with constrained optimization, using the maxnet engine.\n\nImputation (step_impute_median): Ensure complete data for optimization.\nDummy Encoding (step_dummy): Convert categoricals to numeric.\nDate Features (step_date, step_holiday): Create numeric predictors.\nNormalization (step_normalize): Stabilize optimization for feature weights.\nZero Variance (step_zv): Remove uninformative predictors.\nCorrelation (step_corr, optional): Reduce multicollinearity for stability.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Date Features → Dummy Encoding → Sampling → Normalization → Zero Variance → Correlation.\nWhy It Works: These steps ensure clean, numeric data for MaxEnt’s optimization, with step_sample for balanced classes.\nTip: Tune regularization parameters in maxnet. Use step_sample for imbalanced datasets.\n\n\n7.4 Naive Bayes\nGoal: Probabilistic classifier assuming feature independence, using klaR or naivebayes engines.\n\nDiscretization (step_discretize or step_num2factor): Essential for categorical inputs. Tune binning.\nImputation (step_impute_mode): For categorical data.\nDate Features (step_date, step_holiday): Create features to discretize.\nHigh Cardinality (step_other): Avoid zero-frequency issues.\nNear-Zero Variance (step_nzv): Remove low-variance predictors.\nSampling (step_sample): Address class imbalance.\nLaplace Correction: Set laplace &gt; 0 in the model.\n\nStep Order: Imputation → Date Features → Discretization → High Cardinality → Sampling → Near-Zero Variance.\nWhy It Works: Discretization and step_other ensure categorical data, with step_sample for balanced classes.\nTip: Avoid step_corr, as correlated features may be predictive. Tune step_discretize bins.\n\n\n7.5 K-Nearest Neighbors (k-NN)\nGoal: Classify based on nearest neighbors, using the kknn engine.\n\nImputation (step_impute_knn): Aligns with distance-based logic.\nDummy Encoding (step_dummy): Convert categoricals.\nNormalization (step_normalize or step_scale & step_center): Non-negotiable for distance metrics.\nSpatial Features (step_geodist): For spatial data.\nTransformation (step_YeoJohnson): Reduce outliers.\nDimensionality Reduction (step_pca): Mitigate curse of dimensionality. Tune components.\nSampling (step_sample): Address class imbalance.\n\nStep Order: Imputation → Transformation → Dummy Encoding → Spatial Features → Sampling → Dimensionality Reduction → Normalization.\nWhy It Works: Scaling and imputation ensure a meaningful distance metric, with step_sample for balanced classes.\nTip: Tune k and the distance metric. Use step_pca for high-dimensional data."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#key-takeaways-and-practical-tips",
    "href": "posts/05-preprocessing-by-classification-model/main.html#key-takeaways-and-practical-tips",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "8 Key Takeaways and Practical Tips",
    "text": "8 Key Takeaways and Practical Tips\nPreprocessing for classification requires tailoring to each model’s needs and its underlying engine:\n\nModel-Specific Needs: Linear models (e.g., logistic_reg with glmnet) need scaled, numeric data; tree-based models (e.g., rand_forest with ranger) need cardinality control; distance-based models (e.g., svm_rbf with kernlab) and neural networks (e.g., mlp with nnet) require strict scaling.\nStep Order: Impute → Transform → Encode → Sample → Normalize → Remove variance/correlation.\nTune Parameters: Steps like step_discretize, step_ns, or step_pca need tuning via cross-validation (e.g., bins, degrees of freedom, components).\nDomain Knowledge: Use step_interact, step_num2factor, or step_geodist based on data context.\nClass Imbalance: Apply step_sample (SMOTE, ROSE) for imbalanced datasets, especially for boosting, Random Forests, and linear models.\nFeature Selection: Use step_select_roc or step_select_mrmr for high-dimensional data to reduce noise.\nComputational Cost: Prefer step_impute_median or step_impute_knn over step_impute_bag for large datasets. Avoid step_kpca or step_isomap unless necessary.\n\nFinal Tip: Cross-validate with classification metrics (AUC, F1-score, precision, recall) to optimize your pipeline. Save your recipes pipeline for production consistency."
  },
  {
    "objectID": "posts/05-preprocessing-by-classification-model/main.html#wrapping-up",
    "href": "posts/05-preprocessing-by-classification-model/main.html#wrapping-up",
    "title": "A Practical Guide to Preprocessing with R’s recipes Package for Classification Models",
    "section": "9 Wrapping Up",
    "text": "9 Wrapping Up\nThis guide is my go-to reference for preprocessing classification models with recipes, and I hope it helps you tackle your classification projects! I’ll revisit this post on angelfeliz.com as I explore new datasets and models. Share your favorite preprocessing tips in the comments, and let’s keep learning together. Happy classifying!"
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html",
    "href": "posts/02-imputing-missing-values/main.html",
    "title": "Handling Missing Values",
    "section": "",
    "text": "If you are working with real data, it’s normal to find missing values, so it’s very important to understand how to manage them correctly. In R, the default for many functions is to remove missing values to create plots or train machine learning models, but that can be very dangerous as we are adding bias to our analysis that could compromise our final conclusions.\nBut removing values could be a good option if we meet the following statements:\n\nOnly 5% of your dataset’s rows present missing values.\nAll the values have the same probability to be missing. That’s when we say that they are missing completely at random (MCAR).\n\nOtherwise, the best way to handle missing values is to impute values based on general patterns found in the data. If we cannot find patterns in the current data, we will need to find more data until finding a valid pattern to impute the values."
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#introduction",
    "href": "posts/02-imputing-missing-values/main.html#introduction",
    "title": "Handling Missing Values",
    "section": "",
    "text": "If you are working with real data, it’s normal to find missing values, so it’s very important to understand how to manage them correctly. In R, the default for many functions is to remove missing values to create plots or train machine learning models, but that can be very dangerous as we are adding bias to our analysis that could compromise our final conclusions.\nBut removing values could be a good option if we meet the following statements:\n\nOnly 5% of your dataset’s rows present missing values.\nAll the values have the same probability to be missing. That’s when we say that they are missing completely at random (MCAR).\n\nOtherwise, the best way to handle missing values is to impute values based on general patterns found in the data. If we cannot find patterns in the current data, we will need to find more data until finding a valid pattern to impute the values."
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#imputation-practical-example",
    "href": "posts/02-imputing-missing-values/main.html#imputation-practical-example",
    "title": "Handling Missing Values",
    "section": "2 Imputation practical example",
    "text": "2 Imputation practical example\nLet’s assume that all the predictors for a machine learning model are stored in the datasets::airquality data frame, which has some missing values, so we need to explore and decide what to do in this case.\nTo perform all the tasks needed to solve the missing values problems in our dataset, we will load the following packages:\n\n# For modeling and statistical analysis\nlibrary(tidymodels)\n\n# For modeling lasso or elastic-net regression\nlibrary(glmnet)\n\n# For exploring missing values\nlibrary(naniar)\nlibrary(mice)\n\n\n2.1 Confirming if values are MCAR\nBased on the next test, we can reject the null hypothesis and conclude that the missing values aren’t completely at random, so we will need to impute the missing values.\n\nmcar_test(airquality)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      35.1    14 0.00142                4\n\n\n\n\n2.2 Explore missing values patterns\nOnce we know that we need to impute values, it’s important to know that the column with more missing values is the Ozone with 37 values to impute, which we can divide between the ones that can use the rest of the features (35 rows) and the ones that can use all the columns except the Solar.R as they are missing (2 rows).\n\nairquality |&gt;\n  md.pattern() |&gt;\n  invisible()\n\n\n\n\n\n\n\n\n\n\n2.3 Imputing Ozone values\n\n2.3.1 Exploring missing values\nIn the next plot we can see how the missing Ozone values are spread across a wide range of values, so we wouldn’t be able to find big differences between the means of columns with or without missing Ozone values.\nAs we are plotting all variables against the target variable, it’s easy to see that the Temp, Wind and Solar.R present a not linear relation with Ozone and we cannot see a clear pattern for Day and Month.\n\nairquality |&gt;\n  pivot_longer(cols = -Ozone) |&gt;\n  ggplot(aes(value, Ozone))+\n  geom_miss_point(aes(color = is.na(Ozone)),\n                  alpha = 0.5,\n                  show.legend = FALSE)+\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"black\")+\n  scale_color_manual(values = c(\"TRUE\" = \"red\",\n                                \"FALSE\" = \"gray60\"))+\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_light()\n\n\n\n\n\n\n\n\nBased on this result, we know it’s necessary to train a model able to catch the non-linear patterns and omit the Day and Month as they don’t show any relation with Ozone.\n\n\n2.3.2 Training the model to impute\nAs we need to create 2 very similar models (one using the Solar.R column and other without it) it’s better to create a function.\n\nfit_tuned_regression_model &lt;- function(df,\n                                       model_to_tune,\n                                       model_grid,\n                                       formula,\n                                       step_fun,\n                                       seed = 1234){\n  \n  # Defining empty model workflow\n  y_wf &lt;- \n    recipe(formula, data = df) |&gt;\n    step_fun() |&gt;\n    workflow(model_to_tune)\n  \n  # Defining re-samples based on seed\n  set.seed(seed)\n  y_resamples &lt;- mc_cv(df, times = 30)\n  set.seed(NULL)\n  \n  # Fitting re-sample for each grid level\n  y_tuned &lt;- tune_grid(\n    y_wf,\n    resamples = y_resamples,\n    grid = model_grid,\n    metrics = metric_set(yardstick::rsq)\n  )\n\n  # Selecting the best model\n  y_trained_model &lt;- \n    finalize_workflow(y_wf,\n                      select_best(y_tuned)) |&gt;\n    fit(df)\n  \n  # Presenting results\n  results &lt;- list(\"fit\" = y_trained_model,\n                  \"best_fit\" = show_best(y_tuned))\n  \n  print(results$best_fit)\n  \n  return(results)\n  \n}\n\nThen we need to define common inputs for both models.\n\nGlmModel &lt;- linear_reg(\n  engine = \"glmnet\",\n  penalty = tune(),\n  mixture = tune()\n)\n\nGlmGrid &lt;- grid_regular(\n  penalty(),\n  mixture(),\n  levels = 10\n)\n\nozone_steps &lt;- function(recipe){\n  \n  recipe |&gt;\n  step_poly(all_numeric_predictors(),\n            degree = 2) |&gt;\n    step_interact(terms = ~(. -Ozone)^2) |&gt;\n    step_zv(all_numeric_predictors()) |&gt;\n    step_scale(all_numeric_predictors())\n  \n}\n\nNow we can fit each model.\n\nOzoneSolarGlmFitted &lt;- fit_tuned_regression_model(\n  df = na.omit(airquality),\n  model_to_tune = GlmModel,\n  model_grid = GlmGrid,\n  formula = as.formula(\"Ozone ~ Solar.R + Temp + Wind\"),\n  step_fun = ozone_steps,\n  seed = 5050\n)\n\nWarning in select_best(y_tuned): No value of `metric` was given; \"rsq\" will be\nused.\n\n\nWarning in show_best(y_tuned): No value of `metric` was given; \"rsq\" will be\nused.\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 1               1     rsq     standard   0.721    30  0.0159 Preprocessor1_Mo…\n2 1               0.889 rsq     standard   0.721    30  0.0157 Preprocessor1_Mo…\n3 1               0.778 rsq     standard   0.720    30  0.0156 Preprocessor1_Mo…\n4 0.0000000001    0     rsq     standard   0.718    30  0.0165 Preprocessor1_Mo…\n5 0.00000000129   0     rsq     standard   0.718    30  0.0165 Preprocessor1_Mo…\n\nOzoneNotSolarGlmFitted &lt;- airquality |&gt;\n  select(-Solar.R) |&gt;\n  na.omit() |&gt;\n  fit_tuned_regression_model(model_to_tune = GlmModel,\n                         model_grid = GlmGrid,\n                         formula = as.formula(\"Ozone ~ Temp + Wind\"),\n                         step_fun = ozone_steps,\n                         seed = 4518)\n\nWarning in select_best(y_tuned): No value of `metric` was given; \"rsq\" will be used.\nNo value of `metric` was given; \"rsq\" will be used.\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001        0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n2 0.00000000129       0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n3 0.0000000167        0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n4 0.000000215         0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n5 0.00000278          0 rsq     standard   0.654    30  0.0120 Preprocessor1_Mo…\n\n\n\n\n2.3.3 Impute missing values\nOnce we have both models, we can impute the Ozone values, but let’s also create a function to perform this task as we will need to repeat the process very time we need to predict a new value.\n\nimpute_ozone &lt;- function(df,\n                         solar_model,\n                         no_solar_model){\n  \n  mutate(df,\n         Ozone_NA = is.na(Ozone),\n         Ozone = case_when(\n           !is.na(Ozone) ~ Ozone,\n           !is.na(Solar.R)~\n             predict(solar_model, new_data = df)$.pred,\n           TRUE ~\n             predict(no_solar_model, new_data = df)$.pred)\n  )\n}\n\n\nAirOzoneImputed &lt;- impute_ozone(\n  airquality,\n  solar_model = OzoneSolarGlmFitted$fit,\n  no_solar_model = OzoneNotSolarGlmFitted$fit\n)\n\nBy plotting the imputed Ozone values can see that the values follow the patterns present in the non-missing values.\n\nAirOzoneImputed |&gt;\n  pivot_longer(cols = -c(Ozone, Ozone_NA)) |&gt;\n  na.omit() |&gt;\n  ggplot(aes(value, Ozone, color = Ozone_NA))+\n  geom_point(show.legend = FALSE)+\n  scale_color_manual(values = c(\"TRUE\" = \"red\",\n                                \"FALSE\" = \"grey60\"))+\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n\n2.4 Fixing Solar.R values\nOnce we don’t have any missing value in the Ozone column we can explore the remaining 7 missing values in the Solar.R column.\n\nAirOzoneImputed |&gt;\n  md.pattern() |&gt;\n  invisible()\n\n\n\n\n\n\n\n\nNow the missing values represent only 5% of the rows.\n\nnrow(na.omit(AirOzoneImputed)) / nrow(AirOzoneImputed) * 100\n\n[1] 95.42484\n\n\nAnd we don’t have enough data to reject the null hypothesis, and we can not affirm that the missing values are not missing completely at random.\n\nmcar_test(AirOzoneImputed)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      10.3     6   0.114                2\n\n\nSo we can remove the remaining missing values.\n\nAirqualityImputed &lt;- na.omit(AirOzoneImputed)\n\nhead(AirqualityImputed)\n\n  Ozone Solar.R Wind Temp Month Day Ozone_NA\n1    41     190  7.4   67     5   1    FALSE\n2    36     118  8.0   72     5   2    FALSE\n3    12     149 12.6   74     5   3    FALSE\n4    18     313 11.5   62     5   4    FALSE\n7    23     299  8.6   65     5   7    FALSE\n8    19      99 13.8   59     5   8    FALSE"
  },
  {
    "objectID": "posts/02-imputing-missing-values/main.html#final-thoughts",
    "href": "posts/02-imputing-missing-values/main.html#final-thoughts",
    "title": "Handling Missing Values",
    "section": "3 Final thoughts",
    "text": "3 Final thoughts\nI hope this blog can help to improve your ability to handle missing values without compromising your results.\nDon’t forget to explore well your columns as missing values are usually encoded with 0 or 99."
  },
  {
    "objectID": "posts/01-readiable-code/main.html",
    "href": "posts/01-readiable-code/main.html",
    "title": "The Art of Readiable R code",
    "section": "",
    "text": "When we start our journey as programmers it’s normal to get excited by the new possibilities. We get the capacity to do many things that otherwise would be impossible, making projects faster and assuring consistency.\nBut the problems start when you need to modify a script that you wrote 6 months ago. That’s when you find out that you don’t remember why you were applying some specific filters or calculating a value in a odd way.\nAs a Reporting Analyst and I am always creating and changing scripts and after applying the tips provided in The Art of Readable Code by Dustin Boswell and Trevor Foucher I could reduce the time needed to apply changes from 5 to 2 days (60% faster)."
  },
  {
    "objectID": "posts/01-readiable-code/main.html#creating-explicit-names",
    "href": "posts/01-readiable-code/main.html#creating-explicit-names",
    "title": "The Art of Readiable R code",
    "section": "2.1 Creating explicit names",
    "text": "2.1 Creating explicit names\n\n2.1.1 Naming variables\nDefining good variable names is more important than writing a good comment and we should try to give as much context as possible in the variable name. To make this possible:\n\nName based on variable value.\n\nBoolean variables can use words like is, has and should avoid negations. For example: is_integer, has_money and should_end.\nLooping index can have a name followed by the the suffix i. For example: club_i and table_i.\n\nAdd dimensions unit a suffix. For example: price_usd, mass_kg and distance_miles.\nNever change the variable’s value in different sections, instead create a new variable making explicit the change in the name. For example, we can have the variable priceand latter we can create the variable price_discount.\n\nCoding Example\nLet’s apply these points to the mtcars.\n\nmtcars_new_names &lt;-\n  c(\"mpg\" = \"miles_per_gallon\",\n    \"cyl\"= \"cylinders_count\",\n    \"disp\" = \"displacement_in3\",\n    \"hp\" = \"power_hp\",\n    \"drat\" = \"rear_axle_ratio\",\n    \"wt\" = \"weight_klb\",\n    \"qsec\" = \"quarter_mile_secs\",\n    \"vs\" = \"engine_is_straight\",\n    \"am\" = \"transmission_is_manual\",\n    \"gear\" = \"gear_count\",\n    \"carb\" = \"carburetor_count\")\n\nmtcars_renamed &lt;- mtcars\nnames(mtcars_renamed) &lt;- mtcars_new_names[names(mtcars)]\n\nstr(mtcars_renamed)\n\n'data.frame':   32 obs. of  11 variables:\n $ miles_per_gallon      : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cylinders_count       : num  6 6 4 6 8 6 8 4 4 6 ...\n $ displacement_in3      : num  160 160 108 258 360 ...\n $ power_hp              : num  110 110 93 110 175 105 245 62 95 123 ...\n $ rear_axle_ratio       : num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ weight_klb            : num  2.62 2.88 2.32 3.21 3.44 ...\n $ quarter_mile_secs     : num  16.5 17 18.6 19.4 17 ...\n $ engine_is_straight    : num  0 0 1 1 0 1 0 1 1 1 ...\n $ transmission_is_manual: num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear_count            : num  4 4 4 3 3 3 3 4 4 4 ...\n $ carburetor_count      : num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo write good variable names might take some iteration and you might need to play devil’s advocate in under to find out a better name than the initial one.\n\n\n\n\n2.1.2 Defining functions\nCreating explicit functions names can transform a complex process into a simple one.\n\nStart the function with an explicit verb to avoid misunderstandings.\n\n\n\n\nWord\nAlternatives\n\n\n\n\nsend\ndeliver, dispatch, announce, distribute, route\n\n\nfind\nsearch, extract, locate, recover\n\n\nstart\nlaunch, create, begin, open\n\n\nmake\ncreate, set up, build, generate, compose, add, new\n\n\n\n\nThe function name must describe its output.\nA function should do only one thing, otherwise break the functions in more simpler ones to keep the name explicit.\nUse the following words to define range arguments.\n\n\n\n\n\n\n\n\nWord\nUse\n\n\n\n\nmin and max\nUseful to denominate included limits\n\n\nfirst and last\nUseful to denominate exclusive limits\n\n\nbegin and end\nUseful to denominate either inclusive or exclusive limits\n\n\n\nCoding Example\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n  \n  if(!is.data.frame(DF)) stop(\"DF should be a data.frame\")\n  \n  values &lt;- DF[[var_name]]\n  if(!is.numeric(values)) stop(\"var_name should be a numeric column of DF\")\n  \n  min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n  max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n  \n  value_in_range &lt;- \n    values &gt;= min_value & \n    values &lt;= max_value\n  \n  return(DF[value_in_range, ])\n  \n}"
  },
  {
    "objectID": "posts/01-readiable-code/main.html#commenting-correctly",
    "href": "posts/01-readiable-code/main.html#commenting-correctly",
    "title": "The Art of Readiable R code",
    "section": "2.2 Commenting correctly",
    "text": "2.2 Commenting correctly\nThe first step to have a commented project is to have a README file explaining how the code works in a way that to should be enough to present the project to a new team member, but it is also important to add comments to:\n\nExplain how custom functions behave in several situations with minimal examples.\nExplain the reasons behind the decisions that have been taken related to coding style and business logic, like method and constant selection.\nMake explicit pending problems to solve and the initial idea we have to start the solution.\nAvoid commenting bad names, fix them instead.\nSummarize coding sections with a description faster to read than the original code.\n\nCoding Example\nLet’s comment our custom function to explain each point.\n\n# 1. Behavior\n# This function can filter the values of any  data.frame if the var_name\n# is numeric no matter if the column has missing values as it will omit them\n\n# 2. Reasons behind decisions\n# As we are not expecting to make inferences imputation is not necessary.\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n\n  # 5. Reading the code is faster than reading a comment, so we don't need it\n  if(!is.data.frame(DF)) stop(\"DF should be a data.frame\")\n\n  # 2. Reasons behind decisions\n  # We are going to use this vector many times and \n  # saving it as a variable makes the code much easier to read\n  values &lt;- DF[[var_name]]\n  \n  # 5. Reading the code is faster than reading a comment, so we don't need it\n  if(!is.numeric(values)) stop(\"var_name should be a numeric column of DF\")\n  \n  # 2. Reasons behind decisions\n  # Even though a single quantile call could return both values in a vector\n  # it is much simpler to understand if we save each value in a variable\n  min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n  max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n  \n  # 4. The boolean test has an explicit name\n  value_in_range &lt;- \n    values &gt;= min_value & \n    values &lt;= max_value\n  \n  return(DF[value_in_range, ])\n  \n}\n\n\n\n\n\n\n\nNote\n\n\n\nWriting good comments can be challenging, so you better do it in 3 steps:\n\nWrite down whatever comment is on your mind\nRead the comment and see what needs to be improved\nMake the needed improvements"
  },
  {
    "objectID": "posts/01-readiable-code/main.html#code-style",
    "href": "posts/01-readiable-code/main.html#code-style",
    "title": "The Art of Readiable R code",
    "section": "2.3 Code style",
    "text": "2.3 Code style\nIt is important to apply a coding style that make easy to scan the code before going into detail to certain parts. Some advice to improve code style are:\n\nSimilar code should look similar and be grouped in blocks, it will facilitate finding spelling mistakes and prevent repetitive comments.\n\nWe can see how this tips was applied in the keep_rows_in_percentile_range function.\n\nmin_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\nmax_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n\n\nAvoid keeping temporal variables in the global environment .GlobalEnv, instead create a function to make clear the purpose or use pipes (base::|&gt; or magrittr::%&gt;%).\n\nTo ensure this, we created our custom function.\n\nmtcars_renamed |&gt;\n  keep_rows_in_percentile_range(var_name = \"miles_per_gallon\", \n                                min_prob = 0.20,\n                                max_prob = 0.50) |&gt;\n  nrow()\n\n[1] 11\n\n\n\nAvoid writing nested if statements by negating each Boolean test.\n\nIf we hadn’t taken that into consideration our function would be much harder to read.\n\nkeep_rows_in_percentile_range &lt;- function(DF,\n                                          var_name,\n                                          min_prob,\n                                          max_prob){\n  \n  if(is.data.frame(DF)){\n    \n    values &lt;- DF[[var_name]]\n    \n    if(is.numeric(values)){\n      \n      min_value &lt;- quantile(values, na.rm = TRUE, probs = min_prob)\n      max_value &lt;- quantile(values, na.rm = TRUE, probs = max_prob)\n      \n      value_in_range &lt;- \n        values &gt;= min_value & \n        values &lt;= max_value\n      \n      return(DF[value_in_range, ])\n      \n    }else{\n      \n      stop(\"var_name should be a numeric column of DF\")\n      \n    }\n\n  }else{\n    \n    stop(\"DF should be a data.frame\")\n    \n  }\n  \n}\n\n\nBoolean validations should be stored in variables should be stored variables to make explicit what was the test about.\nAlways write constants on the right side of the comparison.\nSimplify Boolean comparisons De Morgan’s Law: ! ( A | B) == !A & !B\n\nIn our custom function min_value and max_value works like constants in comparisons to values\n\nvalue_in_range &lt;- \n  values &gt;= min_value & \n  values &lt;= max_value"
  }
]