---
title: "A Practical Guide to Preprocessing with R’s `recipes` Package for Classification Models"
author: "Angel Feliz"
date: "2025-08-06"
format: 
  html:
    fig-width: 12
    fig-height: 8
    fig-align: "center"
    toc: true
    toc-title: Index
    toc-location: left
    self-contained: true
    number-sections: true
    smooth-scroll: true
image: "data-preprocessing-blog.png"
categories: [Machine Learning, Feature Engineering, Data Preprocessing, R Programming, Recipes Package]
---

If you’re diving into classification tasks with R, you know that preprocessing your data is critical to building effective machine learning models. The `recipes` package is my go-to tool for crafting preprocessing pipelines that prepare data for classification, ensuring models like Random Forests, SVMs, or Naive Bayes can accurately distinguish between classes. In this guide, I’ll walk you through how to use `recipes` to preprocess data for a range of classification models, focusing on binary and multi-class problems. Whether you’re a beginner or a seasoned data scientist, this post is a reference for you (and me!) to revisit when tackling classification challenges.

Why focus on classification? Classification tasks—like predicting whether an email is spam or categorizing customer behavior—are common in data science, and each model has unique data requirements. Preprocessing ensures your data meets these needs, handles class imbalance, and minimizes noise. I’ve included practical tips, clarified step ordering, and balanced computational cost with performance to make this guide actionable. Let’s dive into crafting the perfect preprocessing pipeline for classification!

---

## Why Preprocessing Matters for Classification

Classification models aim to predict discrete class labels (e.g., “positive” vs. “negative” or multiple categories like “low,” “medium,” “high”). Each model has assumptions about the data: logistic regression needs scaled, numeric inputs; tree-based models handle categoricals well but struggle with rare categories; SVMs and neural networks are sensitive to feature scales. A well-designed preprocessing pipeline ensures your data is clean, appropriately formatted, and optimized for class separation, while addressing issues like missing values or imbalanced classes.

The `recipes` package in R lets you define a reproducible sequence of preprocessing steps, like a recipe for your favorite dish. Each step is tailored to your model’s needs, and the order matters—imputation before transformation, transformation before normalization. This guide focuses on classification-specific preprocessing, with tips for handling binary and multi-class problems and evaluating performance with metrics like AUC or F1-score during cross-validation.

**Pro Tip**: Always cross-validate your preprocessing pipeline with your model to ensure robust performance. Use classification metrics (e.g., AUC, precision, recall) to evaluate how well your pipeline supports class separation.

---

## Understanding Model Functions and Their Engines

Before defining a preprocessing strategy, it’s crucial to understand how different classification models function and the engines that power them. This knowledge helps tailor preprocessing steps to each model’s requirements. Below is a table summarizing key classification models and their associated engines, which implement the underlying algorithms.

| Model                  | Engine(s)                             |
|:----------------------|:-------------------------------------|
| C5_rules              | C5.0                                 |
| bag_mars              | earth                                |
| bag_mlp               | nnet                                 |
| bag_tree              | C5.0, rpart                          |
| bart                  | dbarts                               |
| boost_tree            | C5.0, lightgbm, xgboost              |
| decision_tree         | C5.0, partykit, rpart                |
| discrim_flexible      | earth                                |
| discrim_linear        | MASS, mda, sda, sparsediscrim        |
| discrim_quad          | MASS, sparsediscrim                  |
| discrim_regularized   | klaR                                 |
| gen_additive_mod      | mgcv                                 |
| logistic_reg          | brulee, glm, glmnet, LiblineaR, stan |
| mars                  | earth                                |
| maxent                | maxnet                               |
| mlp                   | brulee, brulee_two_layer, nnet       |
| multinom_reg          | brulee, glmnet, nnet                 |
| naive_Bayes           | klaR, naivebayes                     |
| nearest_neighbor      | kknn                                 |
| pls                   | mixOmics                             |
| rand_forest           | aorsf, partykit, randomForest, ranger |
| rule_fit              | xrf                                  |
| svm_linear            | kernlab, LiblineaR                   |
| svm_poly              | kernlab                              |
| svm_rbf               | kernlab, liquidSVM                   |

**Key Insights**:

- **Tree-Based Models (e.g., C5_rules, boost_tree, rand_forest)**: Engines like `C5.0`, `xgboost`, and `ranger` handle categorical variables natively but benefit from cardinality reduction and imputation.
- **Linear Models (e.g., logistic_reg, discrim_linear)**: Engines like `glmnet` and `MASS` require numeric, scaled inputs and are sensitive to multicollinearity.
- **Neural Networks (e.g., mlp, bag_mlp)**: Engines like `nnet` and `brulee` demand strict normalization and complete data.
- **Kernel-Based Models (e.g., svm_rbf, svm_poly)**: Engines like `kernlab` rely on distance metrics, making scaling and transformation critical.
- **Specialized Models (e.g., maxent, gen_additive_mod)**: Engines like `maxnet` and `mgcv` have unique requirements, such as smooth terms or specific feature formats.

Understanding these engines guides preprocessing choices, ensuring compatibility and optimal performance. For example, `C5.0` in `C5_rules` handles missing values internally but benefits from explicit imputation, while `glmnet` in `logistic_reg` requires scaled inputs for regularization.

---

## Tree-Based & Rule-Based Classification Models

Tree-based models (e.g., Random Forests, XGBoost) and rule-based models (e.g., C5.0) split data into regions based on feature values. They’re robust to unscaled data but sensitive to high-cardinality categories and uninformative features. Here’s how to preprocess for classification.

### C5.0 Rules
**Goal**: Generate interpretable IF-THEN rules for classification (e.g., “IF age < 30 AND is_holiday = TRUE THEN positive”). Powered by the `C5.0` engine.

1. **Imputation (`step_impute_mode` or `step_impute_bag`)**: C5.0 can’t handle missing values. Use `step_impute_mode` for categorical variables (simple and fast) or `step_impute_bag` for tree-based imputation that preserves relationships for classification.
2. **Date Features (`step_date`, `step_holiday`)**: Extract features like day of week or holiday flags for rule-based splits (e.g., “IF month = December THEN…”).
3. **Discretization (`step_discretize`)**: Bin numeric variables into categories (e.g., age into “Young,” “Middle-Aged,” “Senior”) to simplify rules and improve generalization. *Tune bin thresholds* to balance granularity and overfitting.
4. **High Cardinality (`step_other`)**: Collapse rare categorical levels into “other” to prevent overly specific rules.
5. **Novel Categories (`step_novel`)**: Add a level for unseen categories in production to avoid errors.
6. **Zero Variance (`step_zv`)**: Remove predictors with one value—they’re useless for rules.
7. **Dummy Encoding (`step_dummy`, optional)**: C5.0 handles categoricals natively, so skip unless sharing the recipe with other models.
8. **Correlation (`step_corr`, optional)**: Remove correlated predictors for simpler rules, but use sparingly as C5.0 is robust to multicollinearity.

**Step Order**: Imputation → Date Features → Discretization → High Cardinality → Novel Categories → Zero Variance → Dummy Encoding → Correlation.

**Why It Works**: These steps create clean, categorical data for interpretable rules, with `step_discretize` and `step_other` preventing overfitting.

**Tip**: Prioritize interpretability by avoiding `step_dummy`. Tune `step_discretize` bins with cross-validation to optimize class separation.

### Bagged MARS (Classification)
**Goal**: Average multiple Multivariate Adaptive Regression Splines (MARS) models for robust class predictions, using the `earth` engine.

MARS fits piecewise linear functions for non-linear class boundaries, with bagging reducing variance. For classification, MARS uses a logistic link.

1. **Imputation (`step_impute_knn`)**: Use `step_impute_knn` to impute based on similar data points, aligning with MARS’s local relationships. Avoid `step_impute_linear` unless data has clear linear trends.
2. **Date and Spatial Features (`step_date`, `step_holiday`, `step_geodist`)**: Create numeric features (e.g., day of year, spatial distances) for MARS splits.
3. **Splines (`step_bs` or `step_ns`)**: Pre-engineer splines to guide MARS’s non-linear splits. *Tune degrees of freedom*.
4. **Interactions (`step_interact`, optional)**: Add specific interactions (e.g., `temperature * humidity`) if domain knowledge suggests they improve class separation.
5. **Normalization (`step_normalize`, optional)**: Stabilizes knot placement but can be skipped for computational efficiency.
6. **Near-Zero Variance (`step_nzv`)**: Remove low-variance predictors to reduce noise.
7. **Correlation (`step_corr`)**: Remove correlated predictors to stabilize MARS’s feature selection.
8. **Sampling (`step_sample`)**: Address class imbalance with SMOTE or ROSE to ensure balanced class predictions.

**Step Order**: Imputation → Date/Spatial Features → Splines → Interactions → Sampling → Normalization → Near-Zero Variance → Correlation.

**Why It Works**: These steps provide clean, numeric data for MARS’s non-linear splits, with `step_sample` ensuring robust handling of imbalanced classes.

**Tip**: Use `step_geodist` only with spatial data. Tune spline parameters and sampling to optimize AUC or F1-score.

### Bagged MLP
**Goal**: Stabilize Multi-Layer Perceptrons (neural networks) for classification by averaging bootstrap samples, using the `nnet` engine.

Preprocessing is identical to a single MLP (see below), as each network needs scaled, numeric data. Bagging reduces variance from random weights.

**Tip**: Bagging increases computational cost—test if the robustness justifies the time for your dataset.

### Bagged Decision Trees
**Goal**: Build an ensemble of decision trees, foundational for Random Forests, using `C5.0` or `rpart` engines.

1. **Imputation (`step_impute_median` or `step_impute_roll`)**: Use `step_impute_median` for robustness in non-temporal data or `step_impute_roll` for time-series.
2. **Date Features (`step_date`, `step_holiday`)**: Create features for tree splits.
3. **High Cardinality (`step_other`)**: Collapse rare categories to improve generalization.
4. **Novel Categories (`step_novel` or `step_unknown`)**: Handle new factor levels.
5. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
6. **Dummy Encoding (`step_dummy`)**: Optional, as some engines (e.g., `catboost`) handle categoricals natively.
7. **Sampling (`step_sample`)**: Use SMOTE or ROSE for class imbalance. *Tune sampling parameters*.
8. **Discretization (`step_cut` or `step_discretize`, optional)**: Pre-bin numerics to reduce noise if domain knowledge supports thresholds.

**Step Order**: Imputation → Date Features → High Cardinality → Novel Categories → Zero Variance → Dummy Encoding → Sampling → Discretization.

**Why It Works**: These steps ensure clean data for tree splits, with `step_sample` critical for imbalanced classes.

**Tip**: Ensure `step_impute_roll` aligns with temporal order. Evaluate sampling with classification metrics like F1-score.

### Boosted Trees (e.g., XGBoost, LightGBM)
**Goal**: Sequentially build trees to correct classification errors, using `C5.0`, `lightgbm`, or `xgboost` engines.

1. **Imputation (`step_impute_median`)**: Explicit imputation stabilizes results, though boosting handles missing values internally.
2. **Date Features (`step_date`, `step_holiday`)**: Create temporal features for splits.
3. **Dummy Encoding (`step_dummy` or `step_dummy_multi_choice`)**: Convert categoricals, with `step_dummy_multi_choice` for multi-label variables.
4. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
5. **High Cardinality (`step_other`)**: Prevent overfitting to rare categories.
6. **Interactions (`step_interact`, optional)**: Add known interactions to guide boosting.
7. **Sampling (`step_sample`)**: Address class imbalance for better class predictions.

**Step Order**: Imputation → Date Features → High Cardinality → Dummy Encoding → Zero Variance → Interactions → Sampling.

**Why It Works**: These steps provide clean data and manage cardinality, with `step_sample` ensuring balanced class learning.

**Tip**: Test if internal missing value handling outperforms imputation. Use `step_dummy_multi_choice` only for multi-label variables.

### Single Decision Tree
**Goal**: Build a single tree for classification, prone to overfitting, using `C5.0`, `partykit`, or `rpart` engines.

1. **Imputation (`step_impute_mode` or `step_impute_median`)**: Use `step_impute_mode` for categoricals, `step_impute_median` for numerics.
2. **Date Features (`step_date`, `step_holiday`)**: Create split-friendly features.
3. **High Cardinality (`step_other`)**: Essential to prevent overfitting.
4. **Novel Categories (`step_unknown`)**: Handle new levels.
5. **Zero/Near-Zero Variance (`step_zv`, `step_nzv`)**: Remove uninformative predictors.
6. **Numeric to Factor (`step_num2factor`, optional)**: Treat discrete numerics as categorical (e.g., number of cylinders).
7. **Spatial Features (`step_depth`, optional)**: Convert coordinates to a single feature.

**Step Order**: Imputation → Date Features → High Cardinality → Novel Categories → Zero/Near-Zero Variance → Numeric to Factor → Spatial Features.

**Why It Works**: These steps reduce overfitting by cleaning data and controlling cardinality.

**Tip**: Single trees are rarely used—consider Random Forests for better performance.

### Random Forest
**Goal**: Build de-correlated trees for robust classification, using `aorsf`, `partykit`, `randomForest`, or `ranger` engines.

1. **Character to Factor (`step_string2factor`)**: Convert character data to factors.
2. **Imputation (`step_impute_median` or `step_impute_bag`)**: Use `step_impute_median` for speed or `step_impute_bag` for complex missingness.
3. **Date Features (`step_date`, `step_holiday`)**: Create temporal features.
4. **High Cardinality (`step_other`)**: Prevent overfitting to rare categories.
5. **Novel Categories (`step_novel`)**: Handle new levels.
6. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
7. **Sampling (`step_sample`)**: Address class imbalance with SMOTE or ROSE.

**Step Order**: Character to Factor → Imputation → Date Features → High Cardinality → Novel Categories → Zero Variance → Sampling.

**Why It Works**: These steps ensure clean, generalizable data, with `step_sample` critical for imbalanced classes.

**Tip**: Test `step_impute_median` vs. `step_impute_bag` for computational efficiency. Use `step_select_roc` for high-dimensional data.

### RuleFit
**Goal**: Combine tree-based rules with LASSO for classification, using the `xrf` engine.

1. **Imputation (`step_impute_median`)**: Ensure complete data for LASSO.
2. **Date Features (`step_date`, `step_holiday`)**: Create base features for rules.
3. **Dummy Encoding (`step_dummy`)**: Convert categoricals for LASSO.
4. **Normalization (`step_normalize`)**: Essential for fair LASSO penalization.
5. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
6. **Correlation (`step_corr`)**: Reduce multicollinearity for LASSO stability.
7. **Interactions (`step_interact`, optional)**: Seed important interactions for LASSO.
8. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Interactions → Dummy Encoding → Normalization → Zero Variance → Correlation → Sampling.

**Why It Works**: These steps balance tree rules and LASSO, with `step_normalize` ensuring fair penalization.

**Tip**: Tune the number of rules in RuleFit. Use `step_sample` for imbalanced classes.

---

## Linear & Generalized Linear Classification Models

Linear models assume a linear relationship between predictors and class probabilities (via a link function). They’re sensitive to scale and multicollinearity.

### Bayesian Additive Regression Trees (BART)
**Goal**: Use Bayesian priors for robust classification with tree ensembles, using the `dbarts` engine.

1. **Imputation (`step_impute_bag`)**: Tree-based imputation aligns with BART’s methodology.
2. **Date Features (`step_date`, `step_holiday`)**: Create features for temporal splits.
3. **Dummy Encoding (`step_dummy`)**: Convert categoricals to numeric.
4. **Normalization (`step_normalize`, optional)**: Aids Bayesian priors and MCMC sampling.
5. **Correlation (`step_corr`, optional)**: Speeds up MCMC by reducing multicollinearity.
6. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Dummy Encoding → Sampling → Normalization → Correlation.

**Why It Works**: These steps provide clean, numeric data, with `step_impute_bag` and `step_sample` enhancing classification performance.

**Tip**: Test if `step_normalize` improves MCMC efficiency. Use `step_sample` for multi-class problems.

### Logistic Regression
**Goal**: Model log-odds for binary classification, using `brulee`, `glm`, `glmnet`, `LiblineaR`, or `stan` engines.

1. **Imputation (`step_impute_median`)**: Ensure complete data with robust imputation.
2. **Date Features (`step_date`, `step_holiday`)**: Create numeric predictors.
3. **Reference Level (`step_relevel`)**: Set reference categories for interpretability.
4. **Dummy Encoding (`step_dummy`)**: Convert categoricals to 0/1.
5. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
6. **Correlation (`step_corr`)**: Stabilize coefficients.
7. **Normalization (`step_normalize` or `step_center` & `step_scale`)**: Essential for regularization and convergence.
8. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Reference Level → Dummy Encoding → Sampling → Normalization → Zero Variance → Correlation.

**Why It Works**: These steps ensure scaled, numeric data for stable logistic regression, with `step_sample` handling imbalance.

**Tip**: Use `step_relevel` for interpretable coefficients. Evaluate with AUC or F1-score.

### Multinomial Logistic Regression
**Goal**: Model multi-class probabilities, using `brulee`, `glmnet`, or `nnet` engines.

1. **Imputation (`step_impute_median`)**: Ensure complete data.
2. **Date Features (`step_date`, `step_holiday`)**: Create covariates.
3. **Reference Level (`step_relevel`)**: Set reference categories.
4. **Dummy Encoding (`step_dummy`, `step_dummy_extract`)**: Handle categoricals and multi-label variables.
5. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
6. **Correlation (`step_corr`)**: Stabilize coefficients.
7. **Normalization (`step_normalize`)**: Essential for regularization.
8. **PLS (`step_pls`, optional)**: Reduce dimensionality for multi-class problems.
9. **Ordinal Scores (`step_ordinalscore`, optional)**: Convert ordered categoricals to numeric.
10. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Reference Level → Dummy Encoding → Ordinal Scores → Sampling → Normalization → Zero Variance → Correlation → PLS.

**Why It Works**: These steps support multi-class classification, with `step_pls` and `step_sample` enhancing performance.

**Tip**: Use `step_dummy_extract` for multi-label variables. Tune `step_pls` components.

### Partial Least Squares (PLS)
**Goal**: Handle multicollinearity in classification, using the `mixOmics` engine.

1. **Imputation (`step_impute_median`)**: Ensure complete data.
2. **Scaling (`step_scale`, `step_center`)**: Essential for covariance calculations.
3. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
4. **Date Features (`step_date`, `step_holiday`)**: Create features for classification.
5. **Near-Zero Variance (`step_nzv`)**: Remove low-variance predictors.
6. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Dummy Encoding → Sampling → Scaling → Near-Zero Variance.

**Why It Works**: These steps ensure scaled, numeric data for PLS’s covariance-based components, with `step_sample` for balanced classes.

**Tip**: Tune the number of PLS components. Use `step_sample` for multi-class problems.

---

## Discriminant Analysis Classification Models

These models model predictor distributions for each class, often assuming normality.

### Flexible Discriminant Analysis (FDA)
**Goal**: Use splines for non-linear class boundaries, using the `earth` engine.

1. **Imputation (`step_impute_knn`)**: Preserves local structure.
2. **Splines (`step_spline_monotone` or `step_ns`)**: Create non-linear boundaries. *Tune degrees of freedom*.
3. **Normalization (`step_normalize` or `step_scale`)**: Ensure comparable basis functions.
4. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
5. **Near-Zero Variance (`step_nzv`)**: Remove low-variance predictors.
6. **Correlation (`step_corr`)**: Reduce multicollinearity.
7. **Transformation (`step_BoxCox` or `step_YeoJohnson`)**: Improve normality for LDA component.
8. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Transformation → Dummy Encoding → Correlation → Splines → Sampling → Normalization → Near-Zero Variance.

**Why It Works**: These steps enable non-linear boundaries while supporting LDA’s normality assumption.

**Tip**: Use `step_YeoJohnson` for flexibility with negative values. Tune spline parameters.

### Linear Discriminant Analysis (LDA)
**Goal**: Assume normal predictors with common covariance, using `MASS`, `mda`, `sda`, or `sparsediscrim` engines.

1. **Imputation (`step_impute_knn`)**: Preserves local structure.
2. **Transformation (`step_BoxCox` or `step_YeoJohnson`)**: Improve normality.
3. **Scaling (`step_center`, `step_scale`)**: Prevent large-variance predictors from dominating.
4. **Correlation (`step_corr` or `step_lincomb`)**: Avoid singular matrices.
5. **Spatial Sign (`step_spatialsign`)**: Robust to outliers.
6. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
7. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Transformation → Dummy Encoding → Correlation → Sampling → Scaling → Spatial Sign.

**Why It Works**: These steps align with LDA’s normality and covariance assumptions.

**Tip**: Check the equal covariance assumption. Use `step_spatialsign` cautiously due to interpretability.

### Quadratic Discriminant Analysis (QDA)
**Goal**: Allow class-specific covariances, using `MASS` or `sparsediscrim` engines.

1. **Imputation (`step_impute_knn`)**: Preserves local structure.
2. **PCA (`step_pca`)**: Stabilize covariance estimation. *Tune components*.
3. **Transformation (`step_BoxCox` or `step_YeoJohnson`)**: Improve normality.
4. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
5. **Correlation (`step_corr`)**: Prevent singular matrices.
6. **Scaling (`step_scale`)**: Ensure fair covariance calculations.
7. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Transformation → Dummy Encoding → Correlation → PCA → Sampling → Scaling → Zero Variance.

**Why It Works**: These steps stabilize QDA’s covariance estimation, with `step_pca` critical for high-dimensional data.

**Tip**: Use `step_pca` for small sample sizes. `step_YeoJohnson` handles negative values.

### Regularized Discriminant Analysis (RDA)
**Goal**: Hybrid of LDA and QDA with shrinkage, using the `klaR` engine.

1. **Imputation (`step_impute_knn`)**: Preserves local structure.
2. **Scaling (`step_center`, `step_scale`)**: Essential for regularization.
3. **Transformation (`step_BoxCox` or `step_YeoJohnson`)**: Improve normality.
4. **Correlation (`step_corr`, `step_lincomb`)**: Prevent singular matrices.
5. **Near-Zero Variance (`step_nzv`)**: Remove low-variance predictors.
6. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
7. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Transformation → Dummy Encoding → Correlation → Sampling → Scaling → Near-Zero Variance.

**Why It Works**: These steps balance LDA and QDA assumptions, with scaling ensuring proper regularization.

**Tip**: Tune RDA’s regularization parameters. Use `step_sample` for multi-class problems.

---

## Neural Networks & Kernel-Based Classification Models

These models are powerful but sensitive to feature scaling.

### Multi-Layer Perceptron (MLP)
**Goal**: Universal approximator for non-linear class boundaries, using `brulee`, `brulee_two_layer`, or `nnet` engines.

1. **Imputation (`step_impute_knn` or `step_impute_bag`)**: Ensure complete data.
2. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
3. **Date Features (`step_date`, `step_holiday`, `step_harmonic`)**: Create numeric features, with `step_harmonic` for cyclical patterns.
4. **Transformation (`step_YeoJohnson`)**: Reduce skewness before normalization.
5. **Normalization (`step_normalize` or `step_scale` & `step_center`)**: Non-negotiable for gradient descent.
6. **Dimensionality Reduction (`step_pca` or `step_ica`)**: Reduce input nodes. *Tune components*.
7. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Transformation → Dummy Encoding → Sampling → Dimensionality Reduction → Normalization.

**Why It Works**: These steps ensure scaled, numeric data for stable training, with `step_sample` for balanced classes.

**Tip**: Use `step_harmonic` for cyclical features. Test `step_ica` for independence assumptions.

### Linear SVM
**Goal**: Find a hyperplane for class separation, using `kernlab` or `LiblineaR` engines.

1. **Imputation (`step_impute_knn`)**: Aligns with SVM’s distance-based nature.
2. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
3. **Scaling (`step_scale`, `step_center`)**: Non-negotiable for distance calculations.
4. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
5. **Correlation (`step_corr`)**: Simplify optimization.
6. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Dummy Encoding → Correlation → Sampling → Scaling → Zero Variance.

**Why It Works**: Scaling ensures fair distance calculations, with `step_sample` for balanced classes.

**Tip**: Evaluate with AUC to optimize the margin.

### Polynomial SVM
**Goal**: Use a polynomial kernel for non-linear class separation, using the `kernlab` engine.

1. **Imputation (`step_impute_knn`)**: More robust than `step_impute_linear`.
2. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
3. **Transformation (`step_YeoJohnson`)**: Reduce skewness before normalization.
4. **Normalization (`step_normalize`)**: Non-negotiable for dot products.
5. **Near-Zero Variance (`step_nzv`)**: Remove low-variance predictors.
6. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Transformation → Dummy Encoding → Sampling → Normalization → Near-Zero Variance.

**Why It Works**: Scaling and transformation ensure a clean feature space for the polynomial kernel.

**Tip**: `step_impute_knn` aligns with the kernel’s non-linear nature. Tune kernel parameters.

### RBF SVM
**Goal**: Use an RBF kernel for non-linear class separation, using `kernlab` or `liquidSVM` engines.

1. **Imputation (`step_impute_knn`)**: Aligns with Euclidean distance.
2. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
3. **Transformation (`step_YeoJohnson`)**: Reduce skew and outliers.
4. **Normalization (`step_normalize`)**: Non-negotiable for distance calculations.
5. **Spatial Features (`step_geodist`, optional)**: For spatial data.
6. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Transformation → Dummy Encoding → Spatial Features → Sampling → Normalization.

**Why It Works**: These steps ensure a scaled feature space for the RBF kernel, with `step_sample` for balanced classes.

**Tip**: Use `step_YeoJohnson` for flexibility with negative values. Tune the gamma parameter.

---

## Other Classification Model Types

### Generalized Additive Models (GAMs)
**Goal**: Model class probabilities with smooth functions, using the `mgcv` engine.

1. **Imputation (`step_impute_knn`)**: Preserves non-linear relationships.
2. **Splines (`step_ns`)**: Create smooth terms for class boundaries. *Tune degrees of freedom*.
3. **Interactions (`step_te`)**: Model smooth interaction surfaces.
4. **Date Features (`step_date`, `step_holiday`)**: Create base features.
5. **Dummy Encoding (`step_dummy`)**: For parametric terms.
6. **Normalization (`step_normalize`, optional)**: For parametric terms.
7. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Splines → Interactions → Dummy Encoding → Sampling → Normalization.

**Why It Works**: Splines and interactions model non-linear class boundaries, with `step_sample` for balanced classes.

**Tip**: Tune spline degrees of freedom. Use `step_sample` for multi-class problems.

### MARS (Classification)
**Goal**: Fit piecewise linear functions for class boundaries, using the `earth` engine.

See Bagged MARS. Steps are identical, but a single MARS model is faster and more prone to overfitting.

**Tip**: Use `step_nzv` and `step_corr` to reduce overfitting. Apply `step_sample` for imbalanced classes.

### Maximum Entropy (MaxEnt)
**Goal**: Model class probabilities with constrained optimization, using the `maxnet` engine.

1. **Imputation (`step_impute_median`)**: Ensure complete data for optimization.
2. **Dummy Encoding (`step_dummy`)**: Convert categoricals to numeric.
3. **Date Features (`step_date`, `step_holiday`)**: Create numeric predictors.
4. **Normalization (`step_normalize`)**: Stabilize optimization for feature weights.
5. **Zero Variance (`step_zv`)**: Remove uninformative predictors.
6. **Correlation (`step_corr`, optional)**: Reduce multicollinearity for stability.
7. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Date Features → Dummy Encoding → Sampling → Normalization → Zero Variance → Correlation.

**Why It Works**: These steps ensure clean, numeric data for MaxEnt’s optimization, with `step_sample` for balanced classes.

**Tip**: Tune regularization parameters in `maxnet`. Use `step_sample` for imbalanced datasets.

### Naive Bayes
**Goal**: Probabilistic classifier assuming feature independence, using `klaR` or `naivebayes` engines.

1. **Discretization (`step_discretize` or `step_num2factor`)**: Essential for categorical inputs. *Tune binning*.
2. **Imputation (`step_impute_mode`)**: For categorical data.
3. **Date Features (`step_date`, `step_holiday`)**: Create features to discretize.
4. **High Cardinality (`step_other`)**: Avoid zero-frequency issues.
5. **Near-Zero Variance (`step_nzv`)**: Remove low-variance predictors.
6. **Sampling (`step_sample`)**: Address class imbalance.
7. **Laplace Correction**: Set `laplace > 0` in the model.

**Step Order**: Imputation → Date Features → Discretization → High Cardinality → Sampling → Near-Zero Variance.

**Why It Works**: Discretization and `step_other` ensure categorical data, with `step_sample` for balanced classes.

**Tip**: Avoid `step_corr`, as correlated features may be predictive. Tune `step_discretize` bins.

### K-Nearest Neighbors (k-NN)
**Goal**: Classify based on nearest neighbors, using the `kknn` engine.

1. **Imputation (`step_impute_knn`)**: Aligns with distance-based logic.
2. **Dummy Encoding (`step_dummy`)**: Convert categoricals.
3. **Normalization (`step_normalize` or `step_scale` & `step_center`)**: Non-negotiable for distance metrics.
4. **Spatial Features (`step_geodist`)**: For spatial data.
5. **Transformation (`step_YeoJohnson`)**: Reduce outliers.
6. **Dimensionality Reduction (`step_pca`)**: Mitigate curse of dimensionality. *Tune components*.
7. **Sampling (`step_sample`)**: Address class imbalance.

**Step Order**: Imputation → Transformation → Dummy Encoding → Spatial Features → Sampling → Dimensionality Reduction → Normalization.

**Why It Works**: Scaling and imputation ensure a meaningful distance metric, with `step_sample` for balanced classes.

**Tip**: Tune k and the distance metric. Use `step_pca` for high-dimensional data.

---

## Key Takeaways and Practical Tips

Preprocessing for classification requires tailoring to each model’s needs and its underlying engine:

- **Model-Specific Needs**: Linear models (e.g., `logistic_reg` with `glmnet`) need scaled, numeric data; tree-based models (e.g., `rand_forest` with `ranger`) need cardinality control; distance-based models (e.g., `svm_rbf` with `kernlab`) and neural networks (e.g., `mlp` with `nnet`) require strict scaling.
- **Step Order**: Impute → Transform → Encode → Sample → Normalize → Remove variance/correlation.
- **Tune Parameters**: Steps like `step_discretize`, `step_ns`, or `step_pca` need tuning via cross-validation (e.g., bins, degrees of freedom, components).
- **Domain Knowledge**: Use `step_interact`, `step_num2factor`, or `step_geodist` based on data context.
- **Class Imbalance**: Apply `step_sample` (SMOTE, ROSE) for imbalanced datasets, especially for boosting, Random Forests, and linear models.
- **Feature Selection**: Use `step_select_roc` or `step_select_mrmr` for high-dimensional data to reduce noise.
- **Computational Cost**: Prefer `step_impute_median` or `step_impute_knn` over `step_impute_bag` for large datasets. Avoid `step_kpca` or `step_isomap` unless necessary.

**Final Tip**: Cross-validate with classification metrics (AUC, F1-score, precision, recall) to optimize your pipeline. Save your `recipes` pipeline for production consistency.

---

## Wrapping Up

This guide is my go-to reference for preprocessing classification models with `recipes`, and I hope it helps you tackle your classification projects! I’ll revisit this post on angelfeliz.com as I explore new datasets and models. Share your favorite preprocessing tips in the comments, and let’s keep learning together. Happy classifying!